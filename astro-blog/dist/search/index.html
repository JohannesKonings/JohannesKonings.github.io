<!DOCTYPE html><html lang="en"> <head><!-- Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/x-icon" href="/favicon.ico"><meta name="generator" content="Astro v4.10.2"><!-- <link rel="preload" href="/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin>
<link rel="preload" href="/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin> --><link rel="preload" href="/fonts/CascadiaMonoNF.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="/fonts/CascadiaMonoNF-SemiBold.woff2" as="font" type="font/woff2" crossorigin><!-- Canonical URL --><link rel="canonical" href="https://johanneskonings.dev/search/"><!-- Primary Meta Tags --><title>Search | Johannes Konings</title><meta name="title" content="Search | Johannes Konings"><meta name="description" content="Search all posts and projects by keyword."><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://johanneskonings.dev/search/"><meta property="og:title" content="Search | Johannes Konings"><meta property="og:description" content="Search all posts and projects by keyword."><meta property="og:image" content="https://johanneskonings.dev/open-graph.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://johanneskonings.dev/search/"><meta property="twitter:title" content="Search | Johannes Konings"><meta property="twitter:description" content="Search all posts and projects by keyword."><meta property="twitter:image" content="https://johanneskonings.dev/open-graph.jpg"><!-- Sitemap --><link rel="sitemap" href="/sitemap-index.xml"><!-- RSS Feed --><link rel="alternate" type="application/rss+xml" title="Search | Johannes Konings" href="https://johanneskonings.dev/rss.xml"><!-- Global Scripts --><script src="/js/theme.js"></script><script src="/js/scroll.js"></script><script src="/js/animate.js"></script><!-- <ViewTransitions  /> --><link rel="stylesheet" href="/_astro/_slug_.COeGf7i0.css"><script type="module" src="/_astro/hoisted.BGfjo5mV.js"></script></head> <body> <header id="header" class="fixed top-0 w-full h-16 z-50" data-astro-cid-3ef6ksr2> <div class="w-full h-full mx-auto px-5 max-w-screen-md">  <div class="relative h-full w-full" data-astro-cid-3ef6ksr2> <div class="absolute left-1/2 top-1/2 -translate-x-1/2 -translate-y-1/2" data-astro-cid-3ef6ksr2> <nav class="hidden md:flex items-center justify-center text-sm gap-1" data-astro-cid-3ef6ksr2> <a href="/" class="h-8 rounded-full px-3 text-current flex items-center justify-center transition-colors duration-300 ease-in-out hover:bg-black/5 dark:hover:bg-white/20 hover:text-black dark:hover:text-white" data-astro-cid-3ef6ksr2> Home </a><a href="/blog" class="h-8 rounded-full px-3 text-current flex items-center justify-center transition-colors duration-300 ease-in-out hover:bg-black/5 dark:hover:bg-white/20 hover:text-black dark:hover:text-white" data-astro-cid-3ef6ksr2> Blog </a> </nav> </div> <div class="buttons absolute right-0 top-1/2 -translate-y-1/2 flex gap-1" data-astro-cid-3ef6ksr2> <a href="/search" aria-label="Search blog posts and projects on Johannes Konings" class="hidden md:flex size-9 rounded-full p-2 items-center justify-center hover:bg-black/5 dark:hover:bg-white/20 stroke-current hover:stroke-black hover:dark:stroke-white border border-black/10 dark:border-white/25 transition-colors duration-300 ease-in-out pointer-events-none bg-black dark:bg-white text-white dark:text-black" data-astro-cid-3ef6ksr2> <svg class="size-full" data-astro-cid-3ef6ksr2> <use href="/ui.svg#search" data-astro-cid-3ef6ksr2></use> </svg> </a> <a href="/rss.xml" target="_blank" aria-label="Rss feed for Johannes Konings" class="hidden md:flex size-9 rounded-full p-2 items-center justify-center bg-transparent hover:bg-black/5 dark:hover:bg-white/20 stroke-current hover:stroke-black hover:dark:stroke-white border border-black/10 dark:border-white/25 transition-colors duration-300 ease-in-out" data-astro-cid-3ef6ksr2> <svg class="size-full" data-astro-cid-3ef6ksr2> <use href="/ui.svg#rss" data-astro-cid-3ef6ksr2></use> </svg> </a> <button id="header-theme-button" aria-label="Toggle light and dark theme" class="hidden md:flex size-9 rounded-full p-2 items-center justify-center bg-transparent hover:bg-black/5 dark:hover:bg-white/20 stroke-current hover:stroke-black hover:dark:stroke-white border border-black/10 dark:border-white/25 transition-colors duration-300 ease-in-out" data-astro-cid-3ef6ksr2> <svg class="size-full block dark:hidden" data-astro-cid-3ef6ksr2> <use href="/ui.svg#sun" data-astro-cid-3ef6ksr2></use> </svg> <svg class="size-full hidden dark:block" data-astro-cid-3ef6ksr2> <use href="/ui.svg#moon" data-astro-cid-3ef6ksr2></use> </svg> </button> <button id="header-drawer-button" aria-label="Toggle drawer open and closed" class="flex md:hidden size-9 rounded-full p-2 items-center justify-center bg-transparent hover:bg-black/5 dark:hover:bg-white/20 stroke-current hover:stroke-black hover:dark:stroke-white border border-black/10 dark:border-white/25 transition-colors duration-300 ease-in-out" data-astro-cid-3ef6ksr2> <svg id="drawer-open" class="size-full" data-astro-cid-3ef6ksr2> <use href="/ui.svg#menu" data-astro-cid-3ef6ksr2></use> </svg> <svg id="drawer-close" class="size-full" data-astro-cid-3ef6ksr2> <use href="/ui.svg#x" data-astro-cid-3ef6ksr2></use> </svg> </button> </div> </div>  </div> </header>  <script>
  function toggleDrawer() {
    const drawer = document.getElementById("drawer");
    const drawerButton = document.getElementById("header-drawer-button");
    drawer?.classList.toggle("open");
    drawerButton?.classList.toggle("open");
  }

  function initializeDrawerButton() {
    const drawerButton = document.getElementById("header-drawer-button");
    drawerButton?.addEventListener("click", toggleDrawer);
  }

  document.addEventListener("astro:after-swap", initializeDrawerButton);
  initializeDrawerButton();
</script> <div id="drawer" class="fixed inset-0 h-0 z-40 overflow-hidden flex flex-col items-center justify-center md:hidden bg-neutral-100 dark:bg-neutral-900 transition-[height] duration-300 ease-in-out" data-astro-cid-hxtyo74s> <nav class="flex flex-col items-center space-y-2" data-astro-cid-hxtyo74s> <a href="/" class="flex items-center justify-center px-3 py-1 rounded-full text-current hover:text-black dark:hover:text-white hover:bg-black/5 dark:hover:bg-white/20 transition-colors duration-300 ease-in-out" data-astro-cid-hxtyo74s> Home </a><a href="/blog" class="flex items-center justify-center px-3 py-1 rounded-full text-current hover:text-black dark:hover:text-white hover:bg-black/5 dark:hover:bg-white/20 transition-colors duration-300 ease-in-out" data-astro-cid-hxtyo74s> Blog </a> </nav> <div class="flex gap-1 mt-5" data-astro-cid-hxtyo74s> <a href="/search" aria-label="Search blog posts and projects on Johannes Konings" class="size-9 rounded-full p-2 items-center justify-center bg-transparent hover:bg-black/5 dark:hover:bg-white/20 stroke-current hover:stroke-black hover:dark:stroke-white border border-black/10 dark:border-white/25 transition-colors duration-300 ease-in-out" data-astro-cid-hxtyo74s> <svg class="size-full" data-astro-cid-hxtyo74s> <use href="/ui.svg#search" data-astro-cid-hxtyo74s></use> </svg> </a> <a href="/rss.xml" target="_blank" aria-label="Rss feed for Johannes Konings" class="size-9 rounded-full p-2 items-center justify-center bg-transparent hover:bg-black/5 dark:hover:bg-white/20 stroke-current hover:stroke-black hover:dark:stroke-white border border-black/10 dark:border-white/25 transition-colors duration-300 ease-in-out" data-astro-cid-hxtyo74s> <svg class="size-full" data-astro-cid-hxtyo74s> <use href="/ui.svg#rss" data-astro-cid-hxtyo74s></use> </svg> </a> <button id="drawer-theme-button" aria-label="Toggle light and dark theme" class="size-9 rounded-full p-2 items-center justify-center bg-transparent hover:bg-black/5 dark:hover:bg-white/20 stroke-current hover:stroke-black hover:dark:stroke-white border border-black/10 dark:border-white/25 transition-colors duration-300 ease-in-out" data-astro-cid-hxtyo74s> <svg class="block dark:hidden size-full" data-astro-cid-hxtyo74s> <use href="/ui.svg#sun" data-astro-cid-hxtyo74s></use> </svg> <svg class="hidden dark:block size-full" data-astro-cid-hxtyo74s> <use href="/ui.svg#moon" data-astro-cid-hxtyo74s></use> </svg> </button> </div> </div>  <main>  <div class="pt-36 pb-5"> <div class="w-full h-full mx-auto px-5 max-w-screen-md">   <div class="animate page-heading"> Search </div>   </div> </div> <div class="flex-1 py-5"> <div class="w-full h-full mx-auto px-5 max-w-screen-md">   <div class="animate"> <style>astro-island,astro-slot,astro-static-slot{display:contents}</style><script>(()=>{var e=async t=>{await(await t())()};(self.Astro||(self.Astro={})).load=e;window.dispatchEvent(new Event("astro:load"));})();;(()=>{var A=Object.defineProperty;var g=(i,o,a)=>o in i?A(i,o,{enumerable:!0,configurable:!0,writable:!0,value:a}):i[o]=a;var d=(i,o,a)=>g(i,typeof o!="symbol"?o+"":o,a);{let i={0:t=>m(t),1:t=>a(t),2:t=>new RegExp(t),3:t=>new Date(t),4:t=>new Map(a(t)),5:t=>new Set(a(t)),6:t=>BigInt(t),7:t=>new URL(t),8:t=>new Uint8Array(t),9:t=>new Uint16Array(t),10:t=>new Uint32Array(t)},o=t=>{let[l,e]=t;return l in i?i[l](e):void 0},a=t=>t.map(o),m=t=>typeof t!="object"||t===null?t:Object.fromEntries(Object.entries(t).map(([l,e])=>[l,o(e)]));class y extends HTMLElement{constructor(){super(...arguments);d(this,"Component");d(this,"hydrator");d(this,"hydrate",async()=>{var b;if(!this.hydrator||!this.isConnected)return;let e=(b=this.parentElement)==null?void 0:b.closest("astro-island[ssr]");if(e){e.addEventListener("astro:hydrate",this.hydrate,{once:!0});return}let c=this.querySelectorAll("astro-slot"),n={},h=this.querySelectorAll("template[data-astro-template]");for(let r of h){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("data-astro-template")||"default"]=r.innerHTML,r.remove())}for(let r of c){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("name")||"default"]=r.innerHTML)}let p;try{p=this.hasAttribute("props")?m(JSON.parse(this.getAttribute("props"))):{}}catch(r){let s=this.getAttribute("component-url")||"<unknown>",v=this.getAttribute("component-export");throw v&&(s+=` (export ${v})`),console.error(`[hydrate] Error parsing props for component ${s}`,this.getAttribute("props"),r),r}let u;await this.hydrator(this)(this.Component,p,n,{client:this.getAttribute("client")}),this.removeAttribute("ssr"),this.dispatchEvent(new CustomEvent("astro:hydrate"))});d(this,"unmount",()=>{this.isConnected||this.dispatchEvent(new CustomEvent("astro:unmount"))})}disconnectedCallback(){document.removeEventListener("astro:after-swap",this.unmount),document.addEventListener("astro:after-swap",this.unmount,{once:!0})}connectedCallback(){if(!this.hasAttribute("await-children")||document.readyState==="interactive"||document.readyState==="complete")this.childrenConnectedCallback();else{let e=()=>{document.removeEventListener("DOMContentLoaded",e),c.disconnect(),this.childrenConnectedCallback()},c=new MutationObserver(()=>{var n;((n=this.lastChild)==null?void 0:n.nodeType)===Node.COMMENT_NODE&&this.lastChild.nodeValue==="astro:end"&&(this.lastChild.remove(),e())});c.observe(this,{childList:!0}),document.addEventListener("DOMContentLoaded",e)}}async childrenConnectedCallback(){let e=this.getAttribute("before-hydration-url");e&&await import(e),this.start()}async start(){let e=JSON.parse(this.getAttribute("opts")),c=this.getAttribute("client");if(Astro[c]===void 0){window.addEventListener(`astro:${c}`,()=>this.start(),{once:!0});return}try{await Astro[c](async()=>{let n=this.getAttribute("renderer-url"),[h,{default:p}]=await Promise.all([import(this.getAttribute("component-url")),n?import(n):()=>()=>{}]),u=this.getAttribute("component-export")||"default";if(!u.includes("."))this.Component=h[u];else{this.Component=h;for(let f of u.split("."))this.Component=this.Component[f]}return this.hydrator=p,this.hydrate},e,this)}catch(n){console.error(`[astro-island] Error hydrating ${this.getAttribute("component-url")}`,n)}}attributeChangedCallback(){this.hydrate()}}d(y,"observedAttributes",["props"]),customElements.get("astro-island")||customElements.define("astro-island",y)}})();</script><script>window._$HY||(e=>{let t=e=>e&&e.hasAttribute&&(e.hasAttribute("data-hk")?e:t(e.host&&e.host.nodeType?e.host:e.parentNode));["click", "input"].forEach((o=>document.addEventListener(o,(o=>{let a=o.composedPath&&o.composedPath()[0]||o.target,s=t(a);s&&!e.completed.has(s)&&e.events.push([s,o])}))))})(_$HY={events:[],completed:new WeakSet,r:{},fe(){}});</script><!--xs--><astro-island uid="Z1a1kc4" data-solid-render-id="s0" component-url="/_astro/Search.AJE-2NEq.js" component-export="default" renderer-url="/_astro/client.BfvbX4EV.js" props="{&quot;data&quot;:[1,[[0,{&quot;id&quot;:[0,&quot;2020-08-18-amplify_env_master_to_main/index.md&quot;],&quot;slug&quot;:[0,&quot;aws-amplify-console-changed-master-branchenv-main&quot;],&quot;body&quot;:[0,&quot;\nAs part of #BlackLivesMatter there was a discussion to rename git branch master to main. How to do that in git and GitHub and some more about the background is described here:\n[http://www.kapwing.com/blog/how-to-rename-your-master-branch-to-main-in-git/](http://www.kapwing.com/blog/how-to-rename-your-master-branch-to-main-in-git/)\n\nTogether with the master branch I had a master env, which was connected to the git master branch in the amplify console. I also wanted to rename this master env.\n\n# AWS Amplify \n\nAfter the master branch was renamed to main, the corresponding env have to be also renamed from master to main.\n\n## Create new env\n\nCheck If you are in the master env with `amplify status`. If not checkout the master env with `amplify env checkout master`.\nRun the command `amplify add env main` and `amplify push`. After that, you can connect the main branch with the main env in the amplify console.\n\n## Connection of main branch with main env in the Amplify Console\n\nThe new main branch and the new main env can now be connected in the amplify console.\n\n![Amplify Console connection branch to env](./amplify_console_connection_branch_to_env1.png)\n\n![Amplify Console connection branch to env](./amplify_console_connection_branch_to_env2.png)\n\nAfterwards the production branch must be changed to main.\n\n![Amplify Console production branch](./amplify_console_production_branch.png)\n\n# Migration\n\nThe new env main is now ready to use, but the data is missing. That must be migrated from the master env to the main env. In my case, it was Cognito users, DynamoDB data and S3 profile photos.\n\n## Cognito\n\nThere are some approaches to export und import cognito users:\n\n[https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-import-users.html](https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-import-users.html)\n\n[https://medium.com/collaborne-engineering/migrate-aws-cognito-user-pools-ff2a91a745a2](https://medium.com/collaborne-engineering/migrate-aws-cognito-user-pools-ff2a91a745a2)\n\nHowever, since it is a small internal app, it is sufficient for users to register again 😊\n\n\n## DynamoDb\n\nThe DynamoDB data is more important. The easiest way to migrate the data was with a shell script and the aws cli: \n\n[dynamodb-migration.sh](https://github.com/JohannesKonings/fff-badminton/blob/main/AmplifyEnvMigration/dynamodb-migration.sh)\n\n## S3\n\nAs described here the users can upload new profile pictures:\n[AWS Amplify Storage: React example for Avatar pictures](https://dev.to/johanneskonings/aws-amplify-storage-react-example-for-avatar-pictures-273o)\n\nThat profile pictures can also be copied by a script [s3-migration.sh](https://github.com/JohannesKonings/fff-badminton/blob/main/AmplifyEnvMigration/s3-migration.sh). But it&#39;s necessary to rename the folder name to the new identity pool identity browser id, that the pictures will be found.\n\n![identity browser]({{ site.baseurl }}/img/2020-08-18-amplify_env_master_to_main/identity_browser.png)\n\nI fixed that manually 😊\n\n\n\n\n\n\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;AWS Amplify Console: How I changed master branchenv to main&quot;],&quot;summary&quot;:[0,&quot;Change of the master branch end amplfiy env to main and migration of the env data&quot;],&quot;date&quot;:[3,&quot;2020-08-18T08:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;aws&quot;],[0,&quot;AWS Amplify&quot;],[0,&quot;AWS Amplify Console&quot;]]]}],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;2020-08-31-example_react_component_props_events.md&quot;],&quot;slug&quot;:[0,&quot;2020-08-31-example_react_component_props_events&quot;],&quot;body&quot;:[0,&quot;\n# ⚠ Disclaimer\n\nThis is a quick example how I pass data to a React component and get the changed data back. If there are better solutions, please let me know.\n\n# Overview\n\nIn different cases it&#39;s neccessary to pass data to a component and get the data back. In this quick example there are two child components. One is used to determine the height of increase in the count. The other one is used to increase the count via a button click with the height of increase from the other component.\n\n![result]({{ site.baseurl }}/img/2020-08-31-example_react_component_props_events/components_overview.png)\n\n# Implementation\n\nThe implementation looks as follows:\n\n## App.js\n\nThe `App.js` contains the two child components `CounterSteps.js` and `Button.js`.\nFrom the `CounterSteps.js` the `App.js` get the height of the increase via a event and store it in the `counterSteps` state. The `counterSteps` value will be passed to `Button.js`. After each time the Button was pressed the `App.js` get the value back.\n\n```javascript\nimport React, { useState } from \&quot;react\&quot;;\nimport CounterSteps from \&quot;./CounterSteps\&quot;;\nimport Button from \&quot;./Button\&quot;;\nimport \&quot;./style.css\&quot;;\n\nexport default function App() {\n  const [counterSteps, setCounterSteps] = useState(0);\n  const [count, setCount] = useState(0);\n\n  const handleCurrentInput = currentInput =&gt; {\n    setCounterSteps(currentInput);\n  };\n\n  const handleCounterIncreased = counterSteps =&gt; {\n    const newCount = count + parseInt(counterSteps);\n    setCount(newCount);\n  };\n\n  return (\n    &lt;div&gt;\n      &lt;h1&gt;Hello StackBlitz!&lt;/h1&gt;\n      &lt;p&gt;current counterStepsInput: {counterSteps}&lt;/p&gt;\n      &lt;p&gt;current count: {count}&lt;/p&gt;\n      &lt;CounterSteps onCurrentInput={handleCurrentInput} /&gt;\n      &lt;Button\n        counterSteps={counterSteps}\n        onCounterIncreased={handleCounterIncreased}\n      /&gt;\n    &lt;/div&gt;\n  );\n}\n```\n## CounterSteps.js\n\nIn the `CounterSteps.js` is a input field. Every change of the value will be passed via a event to the parent component.\n\n```javascript\nimport React, { useState } from \&quot;react\&quot;;\nimport PropTypes from \&quot;prop-types\&quot;;\nimport \&quot;./style.css\&quot;;\n\nexport default function CounterSteps(props) {\n  const [count, setCount] = useState(0);\n  const { onCurrentInput } = props;\n\n  const handleInput = event =&gt; {\n    onCurrentInput(event.target.value);\n  };\n\n  return (\n    &lt;div&gt;\n      &lt;p&gt;\n        &lt;input\n          type=\&quot;number\&quot;\n          name=\&quot;counterSteps\&quot;\n          placeholder=\&quot;counterSteps\&quot;\n          onKeyDown={e =&gt; /[\\+\\-\\.\\,]$/.test(e.key) &amp;&amp; e.preventDefault()}\n          onInput={handleInput}\n        /&gt;\n      &lt;/p&gt;\n    &lt;/div&gt;\n  );\n}\n\nCounterSteps.propTypes = {\n  onCurrentInput: PropTypes.func\n};\n```\n## Button.js\n\nThe `Button.js` receive the height of the inccrease from the parent component. A Button click calls the event and pass the height of increase back. In the `App.js` the total count is calculated.\n\n```javascript\nimport React, { useState, useEffect } from \&quot;react\&quot;;\nimport PropTypes from \&quot;prop-types\&quot;;\nimport \&quot;./style.css\&quot;;\n\nexport default function Button(props) {\n  const [counterSteps, setCounterSteps] = useState(0);\n  const { onCounterIncreased } = props;\n\n  useEffect(() =&gt; {\n    setCounterSteps(props.counterSteps);\n  }, [props]);\n\n  const increaseCount = () =&gt; {\n    onCounterIncreased(counterSteps);\n  };\n\n  return (\n    &lt;div&gt;\n      &lt;button onClick={increaseCount}&gt;increase counter&lt;/button&gt;\n    &lt;/div&gt;\n  );\n}\n\nButton.propTypes = {\n  onCounterIncreased: PropTypes.func\n};\n```\n\n# Result\n\n![result]({{ site.baseurl }}/img/2020-08-31-example_react_component_props_events/result.gif)\n\n# Coding\n\nSee the Coding on GitHub or StackBlitz:\n\n[GitHub](https://github.com/JohannesKonings/example-react-component-props-events)\n\n[StackBlitz](https://stackblitz.com/edit/example-react-component-props-events)\n\n\n\n\n\n\n\n\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;Quick example: pass data to a React component and get the changed value back&quot;],&quot;summary&quot;:[0,&quot;An quick example how to pass data to componenents and get data from components&quot;],&quot;date&quot;:[3,&quot;2020-08-31T08:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;react&quot;],[0,&quot;react components&quot;]]]}],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;2020-10-19-example_react_average_of_items_in_different_arrays.md&quot;],&quot;slug&quot;:[0,&quot;2020-10-19-example_react_average_of_items_in_different_arrays&quot;],&quot;body&quot;:[0,&quot;\n# ⚠ Disclaimer\n\nThis is a quick example how to calculate the average of different items from different arrays. If there are better solutions, please let me know.\n\n# Overview\n\nThe data basis are three fruit baskets with different kinds of fruits an the number ot items in the basket. With a React webpage the several baskets can be choosed and the average of the items will be calculated.\n\n```json\nconst fruitBaskets = [\n  {\n    name: \&quot;fruitBasket1\&quot;,\n    fruitBasket: [\n      { fruitName: \&quot;Apple\&quot;, count: 5 },\n      { fruitName: \&quot;Banana\&quot;, count: 3 },\n      { fruitName: \&quot;Strawberry\&quot;, count: 9 },\n      { fruitName: \&quot;Lemon\&quot;, count: 7 }\n    ]\n  },\n  {\n    name: \&quot;fruitBasket2\&quot;,\n    fruitBasket: [\n      { fruitName: \&quot;Apple\&quot;, count: 5 },\n      { fruitName: \&quot;Banana\&quot;, count: 8 },\n      { fruitName: \&quot;Lemon\&quot;, count: 3 }\n    ]\n  },\n  {\n    name: \&quot;fruitBasket3\&quot;,\n    fruitBasket: [\n      { fruitName: \&quot;Apple\&quot;, count: 5 },\n      { fruitName: \&quot;Banana\&quot;, count: 3 },\n      { fruitName: \&quot;Orange\&quot;, count: 3 },\n      { fruitName: \&quot;Lemon\&quot;, count: 9 },\n      { fruitName: \&quot;Strawberry\&quot;, count: 5 }\n    ]\n  }\n];\n```\n\n# Implementation\n\nThe implementation looks as follows:\n\n## The webpage - App.js\n\nThe `App.js` contains the selection of the fruit baskets via a dropdown and a table for the results. The coding is [here](https://stackblitz.com/edit/example-react-average-of-items-in-different-arrays?file=src%2FApp.js).\n\n![app.js]({{ site.baseurl }}/img/2020-10-19-example_react_average_of_items_in_different_arrays/react_frui_basket_selection.png)\n\n## The calculation - data.js\n\nThe `data.js` contains the data of the fruit baskets and the calculation input data. The coding is [here](https://stackblitz.com/edit/example-react-average-of-items-in-different-arrays?file=src%2Fdata.js).\n\nThe calculation has three steps and is described with the baskets 1 and 2 and is in this format.\n\n```json\n  [\n    [\n      { fruitName: \&quot;Apple\&quot;, count: 5 },\n      { fruitName: \&quot;Banana\&quot;, count: 3 },\n      { fruitName: \&quot;Strawberry\&quot;, count: 9 },\n      { fruitName: \&quot;Lemon\&quot;, count: 7 }\n    ],\n    [\n      { fruitName: \&quot;Apple\&quot;, count: 5 },\n      { fruitName: \&quot;Banana\&quot;, count: 8 },\n      { fruitName: \&quot;Lemon\&quot;, count: 3 }\n    ]\n  ]\n```  \n\n### merge\n\nBasket 1 and 2 are selected. This two arrays will be merged with this coding.\n\n```javascript\n  selectedFruitBaskets.forEach(selectedFruitBasket =&gt; {\n    const found = fruitBaskets.find(\n      fruitBasket =&gt; fruitBasket.name === selectedFruitBasket.name\n    );\n    fruits.push(found.fruitBasket);\n  });\n\n  const basketCounts = fruits.length;\n\n  const mergedBasket = [].concat(...fruits);\n  ```\n\n  **After**\n\n```json\n  [\n    { fruitName: \&quot;Apple\&quot;, count: 5 },\n    { fruitName: \&quot;Banana\&quot;, count: 3 },\n    { fruitName: \&quot;Strawberry\&quot;, count: 9 },\n    { fruitName: \&quot;Lemon\&quot;, count: 7 },\n    { fruitName: \&quot;Apple\&quot;, count: 5 },\n    { fruitName: \&quot;Banana\&quot;, count: 8 },\n    { fruitName: \&quot;Lemon\&quot;, count: 3 }\n  ]\n```\n\n### sum \n\nAt this step the number of fruits of each kind will be summed\n\n```javascript\n  const basketSum = mergedBasket.reduce((acc, curr) =&gt; {\n    if (!acc[curr.fruitName]) {\n      acc[curr.fruitName] = { ...curr, countInBaskets: 1, sum: curr.count };\n      return acc;\n    }\n    acc[curr.fruitName].countInBaskets += 1;\n    acc[curr.fruitName].sum += curr.count;\n\n    return acc;\n  }, {});\n```\n  **After**\n\n```json\n{\n    \&quot;Apple\&quot;: {\n        \&quot;count\&quot;: 5,\n        \&quot;countInBaskets\&quot;: 2,\n        \&quot;fruitName\&quot;: \&quot;Apple\&quot;,\n        \&quot;sum\&quot;: 10\n    },\n    \&quot;Banana\&quot;: {\n        \&quot;count\&quot;: 3,\n        \&quot;countInBaskets\&quot;: 2,\n        \&quot;fruitName\&quot;: \&quot;Banana\&quot;,\n        \&quot;sum\&quot;: 11\n    },\n    \&quot;Lemon\&quot;: {\n        \&quot;count\&quot;: 7,\n        \&quot;countInBaskets\&quot;: 2,\n        \&quot;fruitName\&quot;: \&quot;Lemon\&quot;,\n        \&quot;sum\&quot;: 10\n    },\n    \&quot;Strawberry\&quot;: {\n        \&quot;count\&quot;: 9,\n        \&quot;countInBaskets\&quot;: 1,\n        \&quot;fruitName\&quot;: \&quot;Strawberry\&quot;,\n        \&quot;sum\&quot;: 9\n    }\n}\n```\n\n### average\n\nAfter the summation the average can be calulated.\n\n```javascript\n  const basketAverage = Object.keys(basketSum).map(fruitName =&gt; {\n    const item = basketSum[fruitName];\n    return {\n      fruitName: item.fruitName,\n      averageCountOverall: item.sum / basketCounts,\n      averageCountWithMinOne: item.sum / item.countInBaskets,\n      sum: item.sum\n    };\n  });\n  return basketAverage;\n```\n\n  **After**\n\n```json\n[\n    {\n        \&quot;averageCountOverall\&quot;: 5,\n        \&quot;averageCountWithMinOne\&quot;: 5,\n        \&quot;fruitName\&quot;: \&quot;Apple\&quot;,\n        \&quot;sum\&quot;: 10\n    },\n    {\n        \&quot;averageCountOverall\&quot;: 5.5,\n        \&quot;averageCountWithMinOne\&quot;: 5.5,\n        \&quot;fruitName\&quot;: \&quot;Banana\&quot;,\n        \&quot;sum\&quot;: 11\n    },\n    {\n        \&quot;averageCountOverall\&quot;: 4.5,\n        \&quot;averageCountWithMinOne\&quot;: 9,\n        \&quot;fruitName\&quot;: \&quot;Strawberry\&quot;,\n        \&quot;sum\&quot;: 9\n    },\n    {\n        \&quot;averageCountOverall\&quot;: 5,\n        \&quot;averageCountWithMinOne\&quot;: 5,\n        \&quot;fruitName\&quot;: \&quot;Lemon\&quot;,\n        \&quot;sum\&quot;: 10\n    }\n]\n```\n\n# Result\n\n![result]({{ site.baseurl }}/img/2020-10-19-example_react_average_of_items_in_different_arrays/result.gif)\n\n# Coding\n\nSee the Coding on GitHub or StackBlitz:\n\n[GitHub](https://github.com/JohannesKonings/example-react-average-of-items-in-different-arrays)\n\n[StackBlitz](https://stackblitz.com/edit/example-react-average-of-items-in-different-arrays)\n\n\n\n\n\n\n\n\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;Quick example to get the average of items from different arrays&quot;],&quot;summary&quot;:[0,&quot;Example how to calculate the average of different arrays Javascript (map, reduce, concat, ...)&quot;],&quot;date&quot;:[3,&quot;2020-10-19T06:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;react&quot;],[0,&quot;javascript&quot;]]]}],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;2020-11-27-aws_billing_metrics_and_grafana_cloud.md&quot;],&quot;slug&quot;:[0,&quot;2020-11-27-aws_billing_metrics_and_grafana_cloud&quot;],&quot;body&quot;:[0,&quot;\n# Why Grafana for AWS billing metrics?\n\n[Grafana](https://grafana.com/) is well known for observability dashboards, which can be composed of many data sources.\n\nWith AWS CloudWatch metrics, it is possible to do more or less the same, but it is designed more for one account and more for the AWS metrics.\n\nIn Grafana, it&#39;s easy to use different data sources so that you can have your billing metrics on the same dashboard as your other technical and business metrics from elsewhere.\n\n# Configuration of the Grafana Cloud instance\n\nHere [https://grafana.com/products/cloud/](https://grafana.com/products/cloud/) you can sign up for a Grafana Cloud account.\n\nIt&#39;s free for this use case (and many more :))\n\n![Grafana Cloud pricing]({{ site.baseurl }}/img/2020-11-27-aws_billing_metrics_and_grafana_cloud/grafana_cloud_pricing.png)\n\nAfter login, create a stack with the starter plan.\n\n![Grafana Cloud add stack]({{ site.baseurl }}/img/2020-11-27-aws_billing_metrics_and_grafana_cloud/grafana_cloud_add_stack.png)\n\n![Grafana Cloud starter plan]({{ site.baseurl }}/img/2020-11-27-aws_billing_metrics_and_grafana_cloud/grafana_cloud_starter_plan.png)\n\nThen log in to your instance.\n\n![Grafana Cloud instance login]({{ site.baseurl }}/img/2020-11-27-aws_billing_metrics_and_grafana_cloud/grafana_cloud_instance_login.png)\n\n# Creation of an IAM user for the CloudWatch data source\n\nThe CloudWatch data source needs an access key and a secret key. Therefore is an IAM user needed. As policy can be this used: \n\n[https://grafana.com/docs/grafana/latest/datasources/cloudwatch/#iam-policies](https://grafana.com/docs/grafana/latest/datasources/cloudwatch/#iam-policies)\n\n# Configuration of the CloudWatch Datasource\n\nThe next step is to add the CloudWatch data source to your Grafana cloud instance.\n\n![Grafana Cloud add datasource]({{ site.baseurl }}/img/2020-11-27-aws_billing_metrics_and_grafana_cloud/grafana_cloud_add_datasource.png)\n\n![Grafana Cloud select cloudwatch datasource]({{ site.baseurl }}/img/2020-11-27-aws_billing_metrics_and_grafana_cloud/grafana_cloud_select_cloudwatch_datasource.png)\n\nAs for credentials, use the access key and secret key from the IAM user.\n\n![Grafana Cloud configuration cloudwatch datasource]({{ site.baseurl }}/img/2020-11-27-aws_billing_metrics_and_grafana_cloud/grafana_cloud_configuration_cloudwatch_datasource.png)\n\n# Creation of a dashboard with a graph panel\n\nNow you can use the data source in the panels of a dashboard.\n\n![Grafana Cloud create dashboard]({{ site.baseurl }}/img/2020-11-27-aws_billing_metrics_and_grafana_cloud/grafana_cloud_create_dashboard.png)\n\n![Grafana Cloud add panel]({{ site.baseurl }}/img/2020-11-27-aws_billing_metrics_and_grafana_cloud/grafana_cloud_add_panel.png)\n\nThis is a panel configuration example for the costs of all linked accounts.\n\n![Grafana Cloud accounts panel]({{ site.baseurl }}/img/2020-11-27-aws_billing_metrics_and_grafana_cloud/grafana_cloud_accounts_panel.png)\n\nThis should be considered:\n1. If the CloudWatch data source is not the default one, switch to the CloudWatch data source.\n2. Billing metrics are all in the region us-east-1\n3. The time frame \&quot;this month\&quot; seems the right choice for the monthly costs :)\n\nAnd ready to go 🎉\n\n![Grafana Cloud created dashboard]({{ site.baseurl }}/img/2020-11-27-aws_billing_metrics_and_grafana_cloud/grafana_cloud_created_dashboard.png)\n\n# Usage of a prepared dashboard\n\nHere [https://grafana.com/grafana/dashboards](https://grafana.com/grafana/dashboards) is a big list of official and community dashboards you can import to Grafana.\n\nYou can import a URL or ID from the list or, e.g., this [https://grafana.com/grafana/dashboards/13446](https://grafana.com/grafana/dashboards/13446) Dashboard in that way.\n\n![Grafana Cloud import dashboard]({{ site.baseurl }}/img/2020-11-27-aws_billing_metrics_and_grafana_cloud/grafana_cloud_import_dashboard.png)\n\nThe selection of the data source is required.\n\n![Grafana Cloud import dashboard2]({{ site.baseurl }}/img/2020-11-27-aws_billing_metrics_and_grafana_cloud/grafana_cloud_import_dashboard2.png)\n\nAnd ready to go 🎉\n\n![Grafana Cloud imported dashboard]({{ site.baseurl }}/img/2020-11-27-aws_billing_metrics_and_grafana_cloud/grafana_cloud_imported_dashboard.png)\n\n\n# more dashboards\n\nMonitoring Artist has many more cloud watch dashboards: [https://grafana.com/orgs/monitoringartist](https://grafana.com/orgs/monitoringartist)\n\n\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;How to display AWS billing metrics in Grafana Cloud&quot;],&quot;summary&quot;:[0,&quot;Configuration of Grafana Cloud to display AWS billing metrics&quot;],&quot;date&quot;:[3,&quot;2020-11-27T08:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;aws&quot;],[0,&quot;grafana&quot;]]]}],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;2020-12-06-amplify_admin_ui_user_management.md&quot;],&quot;slug&quot;:[0,&quot;2020-12-06-amplify_admin_ui_user_management&quot;],&quot;body&quot;:[0,&quot;\nIn my web app, I use the authentification [UI Compontents](https://docs.amplify.aws/ui/auth/authenticator/q/framework/react) of Amplify. Without any configuration, this UI component comes with a signup link so that any person who knows the URL can signup for themself. However, this web app is for a certain group of users. That&#39;s why I want to create users who are allowed to sign in.\n\nAnd this is now possible in an easy way via the new Amplify Admin UI.\n\n# The Amplify Admin UI\n\nThe Amplify Admin UI has a big set of functions. You can check the [docs](https://docs.amplify.aws/console/adminui/intro) for more details or test it in a [sandbox](https://sandbox.amplifyapp.com/getting-started).\nThis [post](https://dev.to/aws-builders/aws-amplify-admin-ui-45bm) also gives an excellent overview.\n\n# Hide the signup link\n\nAt first, I had to hide the signup link to avoid that everyone can signup.\n\n![signin with signup]({{ site.baseurl }}/img/2020-12-06-amplify_admin_ui_user_management/signin_with_signup.png)\n\nTo hide the signup link is just one line.\n\n```html\n&lt;AmplifyAuthenticator&gt;\n    &lt;AmplifySignIn hideSignUp=\&quot;true\&quot; slot=\&quot;sign-in\&quot; /&gt;\n&lt;/AmplifyAuthenticator&gt;\n```\nMore options are described in the [docs](https://docs.amplify.aws/ui/auth/authenticator/q/framework/react#sign-in)\n\nThen it looks like that.\n\n![signin without signup]({{ site.baseurl }}/img/2020-12-06-amplify_admin_ui_user_management/signin_without_signup.png)\n\n# Create a new user via Amplify Admin UI\n\nIf a new user is needed, you can create on via the Amplify Admin UI. \nThat you can use the Admin UI, it is just a click in the Amplify [Console](https://console.aws.amazon.com/amplify/home).\n\n![enabled admin ui]({{ site.baseurl }}/img/2020-12-06-amplify_admin_ui_user_management/enabled_admin_ui.png)\n\nIn the Admin UI, it&#39;s in the section user management. Click on \&quot;create user\&quot;.\n\n![user management create user]({{ site.baseurl }}/img/2020-12-06-amplify_admin_ui_user_management/user_management_create_user.png)\n\nType in the user data. The user will get an email with the username and temporary password.\n\n![user management user data]({{ site.baseurl }}/img/2020-12-06-amplify_admin_ui_user_management/user_management_user_data.png)\n\n# User sign in process\n\nThen the user can log in with his username and temporary password.\n\n![user signin tempory password]({{ site.baseurl }}/img/2020-12-06-amplify_admin_ui_user_management/user_signin_tempory_password.png)\n\nAt the next screen, the user has to create a new password.\n\n![user signin new password]({{ site.baseurl }}/img/2020-12-06-amplify_admin_ui_user_management/user_signin_new_password.png)\n\nTwo screens are left. One for the email verification...\n\n![user signin verify]({{ site.baseurl }}/img/2020-12-06-amplify_admin_ui_user_management/user_signin_verify.png)\n\n...and the last one for the verification code.\n\n![user signin verification code]({{ site.baseurl }}/img/2020-12-06-amplify_admin_ui_user_management/user_signin_verification_code.png)\n\nThe last step is not necessary if the admin marks the email as verified.\n\n![admin verify email]({{ site.baseurl }}/img/2020-12-06-amplify_admin_ui_user_management/admin_verify_email.png)\n\nThat&#39;s it 🎉\n\n![user after signin]({{ site.baseurl }}/img/2020-12-06-amplify_admin_ui_user_management/user_after_signin.png)\n\n# Invite User to the Admin UI\n\nNow with the Admin UI, it is also possible to invite users as admins. Even without access to the AWS account.\nSee details [here](https://docs.amplify.aws/console/adminui/access-management).\n\n# Code\n\n[https://github.com/JohannesKonings/fff-badminton](https://github.com/JohannesKonings/fff-badminton)\n\n\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;AWS Amplify Admin UI user management instead of self sign up&quot;],&quot;summary&quot;:[0,&quot;Description of the change from a self sign up to user management from an admin&quot;],&quot;date&quot;:[3,&quot;2020-12-06T08:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;aws&quot;],[0,&quot;AWS Amplify&quot;],[0,&quot;AWS Amplify Admin UI&quot;]]]}],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;2020-12-19-aws_billing_metrics_grafana_cloud_and_terraform.md&quot;],&quot;slug&quot;:[0,&quot;2020-12-19-aws_billing_metrics_grafana_cloud_and_terraform&quot;],&quot;body&quot;:[0,&quot;\n# Why Terraform?\n\nIn this [post]({{ site.baseurl }}/aws/2020/11/27/aws_billing_metrics_and_grafana_cloud/) I described how to display AWS Billing metrics in Grafana Cloud. Therefore it was necessary to create manually the data source and the dashboard.\nWith Terraform, you can describe the setup as code and benefit from the full advantages of [IaC](https://en.wikipedia.org/wiki/Infrastructure_as_code).\n\n# Terraform\n\n[Terraform](https://www.terraform.io/) is a tool for infrastructure as code and works with many different [provider](https://www.terraform.io/docs/providers/index.html#lists-of-terraform-providers).\nTerraform comes with a CLI for the deployments. \n\n# Grafana Provider and Dashboard declaration\n\nFor this use case, you need a Grafana data source and a Grafana dashboard. These configurations have to defined in a .tf file like this [one](https://github.com/JohannesKonings/aws-grafana-billing-dashboard/blob/main/grafana.tf)\n\nAt first the [provider](https://registry.terraform.io/providers/grafana/grafana/latest/docs).\n\n```\nprovider \&quot;grafana\&quot; {\n  url  = var.grafana_url\n  auth = var.grafana_api_key\n}\n```\n\nThen the [data source](https://registry.terraform.io/providers/grafana/grafana/latest/docs/resources/data_source) and [dashboard](https://registry.terraform.io/providers/grafana/grafana/latest/docs/resources/dashboard).\nThe dashboard section links to the file [dashboards/aws-billing.json](https://github.com/JohannesKonings/aws-grafana-billing-dashboard/blob/main/dashboards/aws-billing.json). \n\n```\nresource \&quot;grafana_data_source\&quot; \&quot;cloudwatch\&quot; {\n  type = \&quot;cloudwatch\&quot;\n  name = \&quot;CloudWatch\&quot;\n\n  json_data {\n    default_region = var.region\n    auth_type      = \&quot;keys\&quot;\n  }\n\n  secure_json_data {\n    access_key = var.access_key_grafana\n    secret_key = var.secret_key_grafana\n  }\n}\n\nresource \&quot;grafana_dashboard\&quot; \&quot;metrics\&quot; {\n  config_json = file(\&quot;dashboards/aws-billing.json\&quot;)\n}\n```\n\nFor security reasons and flexible sharing of the template, the parameters for secrets and variables like region are in a .env file. This is the [template](https://github.com/JohannesKonings/aws-grafana-billing-dashboard/blob/main/.env_template) for that.\n\n```\n# The URL of the Grafana instance\nexport TF_VAR_grafana_url=\n# The API key of the Grafana instance\nexport TF_VAR_grafana_api_key=\n# IAM user like described here: https://grafana.com/docs/grafana/latest/datasources/cloudwatch/#iam-policies\nexport TF_VAR_access_key_grafana=\nexport TF_VAR_secret_key_grafana=\n# Default region of the data source\nexport TF_VAR_region=\n```\n\nThe declaration of Terraform variables looks like that.\n\n```\nvariable \&quot;grafana_url\&quot; {}\nvariable \&quot;grafana_api_key\&quot; {}\nvariable \&quot;access_key_grafana\&quot; {}\nvariable \&quot;secret_key_grafana\&quot; {}\nvariable \&quot;region\&quot; {}\n```\n\nIn this case it&#39;s in the file [variable.tf](https://github.com/JohannesKonings/aws-grafana-billing-dashboard/blob/main/variables.tf) like described [here](https://learn.hashicorp.com/tutorials/terraform/azure-variables).\n\n# Grafana API Key\n\nTerraform can \&quot;communicate\&quot; with Grafana via an API key. \nNavigate to this URL \&quot;https://&lt;&lt;Grafana instance&gt;&gt;/org/apikeys\&quot; and create on with the role \&quot;Admin\&quot;.\n\n![grafana api key creation]({{ site.baseurl }}/img/2020-12-19-aws_billing_metrics_grafana_cloud_and_terraform/grafana_api_key_creation.png)\n\nPut the API key into the .env file.\n\n# Usage of the env file\n\nBefore the creation of the S3 Backend and the deployment run the command `source .env`.\n\n# Terraform AWS S3 backend\n\nThis setup so far works for the first deployment. Changes and a redeployment lead to an error because the resource already exists.\nTherefore it&#39;s necessary to extend the setup with a [Terraform backend](https://www.terraform.io/docs/backends/index.html). \nIn this example, it&#39;s a [S3 backend](https://www.terraform.io/docs/backends/types/s3.html).\n\nUnfortunately, it&#39;s not possible to use variables here. This is discussed in this [issue](https://github.com/hashicorp/terraform/issues/13022) with some approaches for workarounds. I use this [one](https://github.com/hashicorp/terraform/issues/13022#issuecomment-482014961), more or less.\n\nConcrete I put a [script](https://github.com/JohannesKonings/aws-grafana-billing-dashboard/blob/main/scripts/terraformInit.sh) around the command `terraform init`. This script can use the environment variables and create a terraform file for the backend.\n\n```bash\n#!/bin/sh\ncat &gt; ./backend.tf &lt;&lt; EOF\nterraform {\n  backend \&quot;s3\&quot; {\n    bucket = \&quot;${TF_VAR_s3_bucket_name}\&quot;\n    key    = \&quot;${TF_VAR_backend_key}\&quot;\n    region = \&quot;${TF_VAR_region}\&quot;\n  }\n}\nEOF\n\nterraform init -input=false\n```\n\nThis [script](https://github.com/JohannesKonings/aws-grafana-billing-dashboard/blob/main/scripts/createS3BackendBucket.sh) creates the bucket.\n\n```sh\n#!/bin/sh\n\naws s3api create-bucket --bucket $TF_VAR_s3_bucket_name --region $TF_VAR_region\n```\n\n\nFor the backend, it needs an IAM user. This [script](https://github.com/JohannesKonings/aws-grafana-billing-dashboard/blob/main/scripts/createUser4S3BackendBucket.sh) creates the user and return access and secret key. Put that into the .env file.\n\n```\n# Name of the Terrafrom S3 backend for state handling\nexport TF_VAR_s3_bucket_name=\n# Name of the state file\nexport TF_VAR_backend_key=terraform.tfstate\n# IAM user credentials to access S3 and write the state file\nexport AWS_ACCESS_KEY_ID=\nexport AWS_SECRET_ACCESS_KEY=\n```\n\n```sh\n#!/bin/sh\n\naws iam create-user --user-name terraform_state\n\naws iam create-access-key --user-name terraform_state\n```\n\nThis [script](https://github.com/JohannesKonings/aws-grafana-billing-dashboard/blob/main/scripts/createAndAttachS3BackendBucketPolicy.sh) creates and attach the missing policy.\n\n```sh\n#!/bin/bash\n\ncat &gt; ./scripts/policy &lt;&lt; EOF\n{\n  \&quot;Version\&quot;: \&quot;2012-10-17\&quot;,\n  \&quot;Statement\&quot;: [\n    {\n      \&quot;Effect\&quot;: \&quot;Allow\&quot;,\n      \&quot;Action\&quot;: \&quot;s3:ListBucket\&quot;,\n      \&quot;Resource\&quot;: \&quot;arn:aws:s3:::${TF_VAR_s3_bucket_name}\&quot;\n    },\n    {\n      \&quot;Effect\&quot;: \&quot;Allow\&quot;,\n      \&quot;Action\&quot;: [\&quot;s3:GetObject\&quot;, \&quot;s3:PutObject\&quot;],\n      \&quot;Resource\&quot;: \&quot;arn:aws:s3:::${TF_VAR_s3_bucket_name}/*\&quot;\n    }\n  ]\n}\nEOF\n\naws iam create-policy --policy-name terraform_state --policy-document file://scripts/policy\n\nARN=$(aws iam list-policies --query &#39;Policies[?PolicyName==`terraform_state`].Arn&#39; --output text)\n\naws iam attach-user-policy --policy-arn $ARN --user-name terraform_state\n```\n\n# Deployment commands\n\nOnce the S3 backend is created, you&#39;re a few commands away from the deployment.\n\nAt first, the initialization of Terraform, which is wrapped in a script.\n\n`sh scripts/terraformInit.sh`\n\nFor the next commands, the Terraform CLI is sufficient.\n\n[validate:](https://www.terraform.io/docs/commands/validate.html) `terraform validate` \n\n[plan:](https://www.terraform.io/docs/commands/plan.html) `terraform plan`\n\n[apply:](https://www.terraform.io/docs/commands/apply.html) `terraform apply`\n\n# Grafana Dasboard changes\n\nThe dashboard can now be changed directly via the JSON file in the folder dashboards. The easier way is to do that manually in Grafana and copy the changed JSON via the share functionality.\n\n![share grafana dashboard]({{ site.baseurl }}/img/2020-12-19-aws_billing_metrics_grafana_cloud_and_terraform/share_grafana_dashboard.png)\n\n![grafana dashboard export view json]({{ site.baseurl }}/img/2020-12-19-aws_billing_metrics_grafana_cloud_and_terraform/grafana_dashboard_export_view_json.png)\n\nOverwrite the file aws-billing.json with the JSON from Grafana and redeploy.\n\n# CI/CD pipeline\n\nThe local deployment is also possible with a CI/CD pipeline. In [this example](https://github.com/JohannesKonings/aws-grafana-billing-dashboard/blob/main/.github/workflows/deploy2grafana.yml) it&#39;s with GitHub actions.\nInstead of the .env file, the variables and credentials coming from GitHub secrets. \n\n# Code\n\n[https://github.com/JohannesKonings/aws-grafana-billing-dashboard](https://github.com/JohannesKonings/aws-grafana-billing-dashboard)\n\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;How to setup AWS Billing metrics in Grafana Cloud via Terraform&quot;],&quot;summary&quot;:[0,&quot;Terraform setup to display AWS billing metrics in Grafana Cloud with S3 Backend and CI/CD pipeline&quot;],&quot;date&quot;:[3,&quot;2020-12-19T07:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;aws&quot;],[0,&quot;grafana&quot;],[0,&quot;terraform&quot;]]]}],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;2021-02-28-github_automatic_releases_and changelog.md&quot;],&quot;slug&quot;:[0,&quot;2021-02-28-github_automatic_releases_and-changelog&quot;],&quot;body&quot;:[0,&quot;\n# What are GitHub actions?\n\nThat is well described in this [post](https://dev.to/github/what-are-github-actions-3pml) by [Brian Douglas](https://dev.to/bdougieyo).\nHe has also an entire [post series](https://dev.to/github/28-days-of-github-action-tips-4opg) about tips around GitHub actions.\n\n# Which problem should be solved?\n\nThere are different solutions to create automatic releases based on certain criteria. Again [Brian Douglas](https://dev.to/bdougieyo) pointed out some possibilities in this [post](https://dev.to/github/generate-semantic-release-with-github-actions-2lll).\n\nFor [this](https://github.com/abap-observability-tools) open-source project, the requirement was to determine the release structure via labels at the pull request. This was preferred over [conventional commits](https://www.conventionalcommits.org/en/v1.0.0/).\nAlso, not every merged pull request should automatically trigger a new release. Therefore, a draft is the right way to collect the changes and publish a version whenever needed.\n\nAddionatylly to the GitHub releases, a changelog.md helps see the version history, for example, directly in the code editor.\nThat changelog.md should be updated every time a release is published.\n\nThe combination [Release Drafter](https://github.com/release-drafter/release-drafter) and [gren](https://github.com/github-tools/github-release-notes) is the approach to solve the problem.\n\n# Configure Release Drafter\n\nTo configure [Release Drafter](https://github.com/release-drafter/release-drafter) in the default way, it needs two files and the according labels.\n\nThis [template](https://github.com/abap-observability-tools/abap-log-exporter/blob/main/.github/release-drafter.yml) describes the structure of the release draft and the needed labels.\nThe full path is `.github/release-drafter.yml`\n\n```yaml\nname-template: &#39;v$RESOLVED_VERSION 🌈&#39;\ntag-template: &#39;v$RESOLVED_VERSION&#39;\ncategories:\n  - title: &#39;🚀 Features&#39;\n    labels:\n      - &#39;feature&#39;\n      - &#39;enhancement&#39;\n  - title: &#39;🐛 Bug Fixes&#39;\n    labels:\n      - &#39;fix&#39;\n      - &#39;bugfix&#39;\n      - &#39;bug&#39;\n  - title: &#39;🧰 Maintenance&#39;\n    label: &#39;chore&#39;\n  - title: &#39;🧺 Miscellaneous&#39; #Everything except ABAP\n    label: &#39;misc&#39;\nchange-template: &#39;- $TITLE @$AUTHOR (#$NUMBER)&#39;\nchange-title-escapes: &#39;\\&lt;*_&amp;&#39; # You can add # and @ to disable mentions, and add ` to disable code blocks.\nversion-resolver:\n  major:\n    labels:\n      - &#39;major&#39;\n  minor:\n    labels:\n      - &#39;minor&#39;\n  patch:\n    labels:\n      - &#39;patch&#39;\n  default: patch\ntemplate: |\n  ## Changes\n  $CHANGES\n```\n\nThe GitHub actions [configuration](https://github.com/abap-observability-tools/abap-log-exporter/blob/main/.github/workflows/release-drafter.yml) like this:\n\n`github/workflows/release-drafter.yml`\n\n```yaml\nname: Release Drafter\n\non:\n  push:\n    # branches to consider in the event; optional, defaults to all\n    branches:\n      - main\n\njobs:\n  update_release_draft:\n    runs-on: ubuntu-latest\n    steps:\n      # Drafts your next Release notes as Pull Requests are merged into \&quot;master\&quot;\n      - uses: release-drafter/release-drafter@v5\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n```\n# Configure gren\n\nThe releases are published manually at certain times. This trigger [this](https://github.com/abap-observability-tools/abap-log-exporter/blob/main/.github/workflows/update-changelog.yml) configuration.\n\n```yaml\nname: \&quot;update changelog\&quot;\non:\n  release:\n    types: [published]\n\njobs:\n  update-changelog:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n\n    - name: Update changelog\n      run: |\n        npm install github-release-notes\n        export GREN_GITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }}\n        npm run overrideChangelog\n    - name: Create Pull Request\n      uses: peter-evans/create-pull-request@v3\n      with:\n        commit-message: update changelog\n        title: Update Changelog\n        body: Update changelog to reflect release changes\n        branch: update-changelog\n        base: main\n```\n\nThe command `\&quot;overrideChangelog\&quot;: \&quot;gren changelog --override\&quot;` from the [package.json](https://github.com/abap-observability-tools/abap-log-exporter/blob/main/package.json) update then the changelog.md.\n\nBecause of the main branch protection, it&#39;s not possible to push the changes directly back. This will do this via a pull request with the GitHub action [create-pull-request](https://github.com/marketplace/actions/create-pull-request).\n\n# How it looks like\n\nThe collection of the changes in a release draft.\n\n![release-draft]({{ site.baseurl }}/img/2021-02-28-github_automatic_releases_and changelog/release-draft.png)\n\nThe labels in a pull request.\n\n![label-pull-request]({{ site.baseurl }}/img/2021-02-28-github_automatic_releases_and changelog/label-pull-request.png)\n\nThe result in the GitHub release.\n\n![minor-enhancement]({{ site.baseurl }}/img/2021-02-28-github_automatic_releases_and changelog/minor-enhancement.png)\n\nThe result in the CHANGELOG.md.\n\n![changelog]({{ site.baseurl }}/img/2021-02-28-github_automatic_releases_and changelog/changelog.png)\n\n\n# Code\n\n[https://github.com/abap-observability-tools](https://github.com/abap-observability-tools)\n\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;GitHub actions example for automatic release drafts and changelog.md creation&quot;],&quot;summary&quot;:[0,&quot;This post is how to define your release draft via labels in pull requests and the update of the changelog.md after publishing a release&quot;],&quot;date&quot;:[3,&quot;2021-02-28T07:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;github&quot;],[0,&quot;github actions&quot;]]]}],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;2021-08-27-aws_example_ddb_analytics.md&quot;],&quot;slug&quot;:[0,&quot;2021-08-27-aws_example_ddb_analytics&quot;],&quot;body&quot;:[0,&quot;\n# Why?\n\nThe data of a DynamoDb table is not so easy to analyze as a [RDS](https://aws.amazon.com/rds/) with e.g., the [pgAdmin](https://www.pgadmin.org/).\nIt will be somehow possible with scan operation but it&#39;s in the most cases [not recommented](https://dynobase.dev/dynamodb-scan/).\n\nAnother possibility is the [export to S3 functionylity](https://aws.amazon.com/blogs/aws/new-export-amazon-dynamodb-table-data-to-data-lake-amazon-s3/).\n\nIn this post, it&#39;s described to use streams. Since 11/2020, it is also possible [to use kinesis data streams](https://aws.amazon.com/about-aws/whats-new/2020/11/now-you-can-use-amazon-kinesis-data-streams-to-capture-item-level-changes-in-your-amazon-dynamodb-table/) for such a case.\n\nThat also allows to analyze changes and use it for audits.\n\nA example with DynamoDb streams are here:\n- [https://www.youtube.com/watch?v=7QFUEh-FYYE](https://www.youtube.com/watch?v=7QFUEh-FYYE)\n- [https://www.youtube.com/watch?v=17AmrTqn0GY](https://www.youtube.com/watch?v=17AmrTqn0GY)\n\n\n# Architecture\n\n![architecture](https://raw.githubusercontent.com/JohannesKonings/test-aws-dynamodb-athena-tf/main/diagrams/overview.png)\n\nThe lambda is sending fake person data to DynamoDb. The integration of the Kinesis Data Stream into the DynamoDb is connected to the Kinesis Firehose, which sends the changes partitioned to the S3 bucket.\n\nThe Glue crawler will recognize the data structure and create a table, which can be accessed from Athena to analyze the data.\n\nLet&#39;s see the certain building blocks\n\n# Lambda (for data creation)\n\nThe [Lambda](https://github.com/JohannesKonings/test-aws-dynamodb-athena-tf/blob/main/terraform/lambda.tf) is created with a module from [serverless.tf](serverless.tf).\n\nThe source code is [here](https://github.com/JohannesKonings/test-aws-dynamodb-athena-tf/blob/main/terraform/src/persons-loader/index.js)\n\nThe number of created persons depends on the test event.\n\n```json\n{\n  \&quot;batchSize\&quot;: 5\n}\n```\n\n# DynamoDb and Kinesis Data Stream\n\n[This](https://github.com/JohannesKonings/test-aws-dynamodb-athena-tf/blob/main/terraform/dynamodb.tf) is the creation of the DynamoDb with the Kinesis Data Stream.\n\n```terraform\nresource \&quot;aws_dynamodb_table\&quot; \&quot;aws_dynamodb_table\&quot; {\n  name         = var.TABLE_NAME\n  billing_mode = \&quot;PAY_PER_REQUEST\&quot;\n  hash_key     = \&quot;pk\&quot;\n\n  attribute {\n    name = \&quot;pk\&quot;\n    type = \&quot;S\&quot;\n  }\n}\n\nresource \&quot;aws_kinesis_stream\&quot; \&quot;aws_kinesis_stream\&quot; {\n  name            = \&quot;${var.TABLE_NAME}-data-stream\&quot;\n  shard_count     = 1\n  encryption_type = \&quot;KMS\&quot;\n  kms_key_id      = aws_kms_key.aws_kms_key.arn\n}\n\nresource \&quot;aws_dynamodb_kinesis_streaming_destination\&quot; \&quot;aws_dynamodb_kinesis_streaming_destination\&quot; {\n  stream_arn = aws_kinesis_stream.aws_kinesis_stream.arn\n  table_name = aws_dynamodb_table.aws_dynamodb_table.name\n}\n```\n\nThat adds to the DynamoDb, a Kinesis Data Stream, and connects it to the DynamoDb.\n\n![kinesis data stream]({{ site.baseurl }}/img/2021-08-27-aws_example_ddb_analytics/kinesis_data_stream.png)\n\n![kinesis data stream ddb]({{ site.baseurl }}/img/2021-08-27-aws_example_ddb_analytics/kinesis_data_stream_ddb.png)\n\n# Kinesis Data Firehose and S3 Bucket\n\nKinesis Data Firehose is the connection between the Kinesis Data Stream to the S3 Bucket.\n\nUnfortunately, Firehose stores the JSONs without a linefeed. Therefore it&#39;s a lambda for conversion is necessary.\n\nMore about that is described in this [post](https://medium.com/analytics-vidhya/append-newline-to-amazon-kinesis-firehose-json-formatted-records-with-python-f58498d0177a)\n\nBesides policy configuration, it looks like this.\n\n```terraform\nresource \&quot;aws_kinesis_firehose_delivery_stream\&quot; \&quot;aws_kinesis_firehose_delivery_stream\&quot; {\n  name        = local.firehose-name\n  destination = \&quot;extended_s3\&quot;\n\n  kinesis_source_configuration {\n    kinesis_stream_arn = aws_kinesis_stream.aws_kinesis_stream.arn\n    role_arn           = aws_iam_role.aws_iam_role.arn\n  }\n\n  extended_s3_configuration {\n    role_arn   = aws_iam_role.aws_iam_role.arn\n    bucket_arn = aws_s3_bucket.aws_s3_bucket.arn\n\n    processing_configuration {\n      enabled = \&quot;true\&quot;\n\n      processors {\n        type = \&quot;Lambda\&quot;\n\n        parameters {\n          parameter_name  = \&quot;LambdaArn\&quot;\n          parameter_value = \&quot;${module.lambda_function_persons_firehose_converter.lambda_function_arn}:$LATEST\&quot;\n        }\n      }\n    }\n\n    cloudwatch_logging_options {\n      enabled         = true\n      log_group_name  = aws_cloudwatch_log_group.aws_cloudwatch_log_group_firehose.name\n      log_stream_name = aws_cloudwatch_log_stream.aws_cloudwatch_log_stream_firehose.name\n    }\n  }\n}\n```\nDetails are [here](https://github.com/JohannesKonings/test-aws-dynamodb-athena-tf/blob/main/terraform/kinesis_firehose.tf)\n\nThe delivery of the data to the S3 bucket is buffered. Here are the default values.\n\n![firehose-buffer]({{ site.baseurl }}/img/2021-08-27-aws_example_ddb_analytics/firehose_buffer.png)\n\n# Glue crawler\n\nAthena needs a structured table for the SQL queries. The Glue crawler creates this from the data in the S3 bucket.\n\n```terraform\nresource \&quot;aws_glue_crawler\&quot; \&quot;aws_glue_crawler\&quot; {\n  database_name = aws_glue_catalog_database.aws_glue_bookings_database.name\n  name          = local.glue-crawler-name\n  role          = aws_iam_role.aws_iam_role_glue_crawler.arn\n\n  configuration = jsonencode(\n    {\n      \&quot;Version\&quot; : 1.0\n      CrawlerOutput = {\n        Partitions = { AddOrUpdateBehavior = \&quot;InheritFromTable\&quot; }\n      }\n    }\n  )\n\n  s3_target {\n    path = \&quot;s3://${aws_s3_bucket.aws_s3_bucket.bucket}\&quot;\n  }\n}\n```\n\nDetails [here](https://github.com/JohannesKonings/test-aws-dynamodb-athena-tf/blob/main/terraform/glue.tf)\n\nFor test purposes, it&#39;s enough to run the crawler before any analysis. Scheduling is also possible.\n\n![glue-run-crawler]({{ site.baseurl }}/img/2021-08-27-aws_example_ddb_analytics/glue_run_crawler.png)\n\nThat creates this table, which is accessible by Athena.\n\n![glue-table]({{ site.baseurl }}/img/2021-08-27-aws_example_ddb_analytics/glue_table.png)\n\n# Athena\n\nFor Athena it needs an S3 bucket for the query results and, for better isolation to other projects, a workgroup.\n\n```terraform\nlocals {\n  athena-query-results-s3-name = \&quot;${var.TABLE_NAME}-query-results\&quot;\n  athena-workgroup-name        = \&quot;${var.TABLE_NAME}-workgroup\&quot;\n}\nresource \&quot;aws_s3_bucket\&quot; \&quot;aws_s3_bucket_bookings_query_results\&quot; {\n  bucket = local.athena-query-results-s3-name\n  acl    = \&quot;private\&quot;\n\n  versioning {\n    enabled = true\n  }\n\n  server_side_encryption_configuration {\n    rule {\n      apply_server_side_encryption_by_default {\n        kms_master_key_id = aws_kms_key.aws_kms_key.arn\n        sse_algorithm     = \&quot;aws:kms\&quot;\n      }\n    }\n  }\n}\n\nresource \&quot;aws_athena_workgroup\&quot; \&quot;aws_athena_workgroup\&quot; {\n  name = local.athena-workgroup-name\n\n  configuration {\n    enforce_workgroup_configuration    = true\n    publish_cloudwatch_metrics_enabled = true\n\n    result_configuration {\n      output_location = \&quot;s3://${aws_s3_bucket.aws_s3_bucket_bookings_query_results.bucket}/output/\&quot;\n\n      encryption_configuration {\n        encryption_option = \&quot;SSE_KMS\&quot;\n        kms_key_arn       = aws_kms_key.aws_kms_key.arn\n      }\n    }\n  }\n}\n```\nDetails [here](https://github.com/JohannesKonings/test-aws-dynamodb-athena-tf/blob/main/terraform/glue.tf)\n\n## Analysis\n\nFirst select the new workgroup.\n\n![athena-workgroup]({{ site.baseurl }}/img/2021-08-27-aws_example_ddb_analytics/athena_workgroup.png)\n\nAnd than the new Database.\n\n![athena-database]({{ site.baseurl }}/img/2021-08-27-aws_example_ddb_analytics/athena_database.png)\n\n\n### Query example\n\nDynamoDb sends the changes of an item as INSERT, MODIFY or REMOVE. To the current data of the table, this Query will work.\n\n```SQL\nSELECT dynamodb.newimage.pk.s AS pk,\n         dynamodb.newimage.person.M.firstname.s AS firstname,\n         dynamodb.newimage.person.M.lastname.s AS lastname,\n         dynamodb.approximatecreationdatetime AS ts,\n         dynamodb.newimage,\n         *\nFROM \&quot;persons-db\&quot;.\&quot;persons_firehose_s3_bucket\&quot; AS persons1\nWHERE (eventname = &#39;INSERT&#39;\n        OR eventname = &#39;MODIFY&#39;)\n        AND dynamodb.approximatecreationdatetime =\n    (SELECT MAX(dynamodb.approximatecreationdatetime)\n    FROM \&quot;persons-db\&quot;.\&quot;persons_firehose_s3_bucket\&quot; AS persons2\n    WHERE persons2.dynamodb.newimage.pk.s = persons1.dynamodb.newimage.pk.s);\n```\n\n![athena-ddb]({{ site.baseurl }}/img/2021-08-27-aws_example_ddb_analytics/athena_ddb.png)\n\n# Cost Alert 💰\n\n⚠️ Don&#39;t forget to destroy after testing. Kinesis Data Streams has [costs](https://aws.amazon.com/kinesis/data-streams/pricing/) per hour\n\n\n# Code\n\n[https://github.com/JohannesKonings/test-aws-dynamodb-athena-tf](https://github.com/JohannesKonings/test-aws-dynamodb-athena-tf)\n\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;Example how to analyze DynamoDB item changes with Kinesis and Athena created with Terraform&quot;],&quot;summary&quot;:[0,&quot;This post is how stream data changes of a DynamoDb table via Kinesis Data Stream and Kinesis Firehose to S3, and analyze the data with Athena&quot;],&quot;date&quot;:[3,&quot;2021-08-27T06:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;aws&quot;],[0,&quot;aws kinesis&quot;],[0,&quot;aws athena&quot;],[0,&quot;terraform&quot;]]]}],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;2021-10-26-aws_example_ddb_analytics_cdk.md&quot;],&quot;slug&quot;:[0,&quot;2021-10-26-aws_example_ddb_analytics_cdk&quot;],&quot;body&quot;:[0,&quot;\nThis is the same like described [here]({{ site.baseurl }}/aws/2021/08/27/aws_example_ddb_analytics/), but instead of terraform it&#39;s build with [CDK](https://aws.amazon.com/cdk/).\n\nTo bootstrap the project run this command: `cdk init app --language typescript`\nFurther information are [here](https://docs.aws.amazon.com/cdk/latest/guide/hello_world.html)\n\nAll the services are in [this](https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk/blob/main/cdk/lib/cdk-stack.ts) file.\n\n## Updates\n\n2022-09-11: Add prefix for kinesis data firehose S3 destination\n2022-08-13: CDK migrated to v2\n\n## KMS key\n\nThis creates are KMS key with an alias to encrypt the data in the created services.\n\n```typescript\nconst kmsKey = new kms.Key(this, &#39;kmsKey&#39;, {\n      enableKeyRotation: true,\n    })\n\nkmsKey.addAlias(name)\n```\n\n## DynamoDb and Kinesis Data Stream\n\nThis is the creation of the DynamoDb with the Kinesis Data Stream.\n\n```typescript\nconst stream = new kinesis.Stream(this, &#39;Stream&#39;, {\n      streamName: `${name}-data-stream`,\n      encryption: kinesis.StreamEncryption.KMS,\n      encryptionKey: kmsKey,\n    })\n\n    const table = new dynamodb.Table(this, &#39;Table&#39;, {\n      tableName: name,\n      partitionKey: { name: &#39;pk&#39;, type: dynamodb.AttributeType.STRING },\n      billingMode: dynamodb.BillingMode.PAY_PER_REQUEST,\n      encryption: dynamodb.TableEncryption.CUSTOMER_MANAGED,\n      encryptionKey: kmsKey,\n      kinesisStream: stream,\n    })\n```\n\nThat adds to the DynamoDb, a Kinesis Data Stream, and connects it to the DynamoDb.\n\n![kinesis data stream]({{ site.baseurl }}/img/2021-10-26-aws_example_ddb_analytics_cdk/kinesis_data_stream.png)\n\n![kinesis data stream ddb]({{ site.baseurl }}/img/2021-10-26-aws_example_ddb_analytics_cdk/kinesis_data_stream_ddb.png)\n\n## Kinesis Data Firehose and S3 Bucket\n\nKinesis Data Firehose is the connection between the Kinesis Data Stream to the S3 Bucket.\n\nUnfortunately, Firehose stores the JSONs without a linefeed. Therefore it&#39;s a lambda for conversion is necessary.\n\nMore about that is described in this [post](https://medium.com/analytics-vidhya/append-newline-to-amazon-kinesis-firehose-json-formatted-records-with-python-f58498d0177a)\n\nTo have the kinesis firehose data isolated under a \&quot;namespace\&quot; we use a prefix.\n\nIt looks like this.\n\n```typescript\n const firehoseBucket = new s3.Bucket(this, &#39;firehose-s3-bucket&#39;, {\n      bucketName: `${name}-firehose-s3-bucket`,\n      encryptionKey: kmsKey,\n    })\n\nconst processor = new lambda.NodejsFunction(this, &#39;lambda-function-processor&#39;, {\n  functionName: `${name}-firehose-converter`,\n  timeout: cdk.Duration.minutes(2),\n  bundling: {\n    sourceMap: true,\n  },\n})\n\nconst lambdaProcessor = new LambdaFunctionProcessor(processor, {\n  retries: 5,\n})\n\nconst ddbChangesPrefix = &#39;ddb-changes&#39;;\n\nconst s3Destination = new destinations.S3Bucket(firehoseBucket, {\n  encryptionKey: kmsKey,\n  bufferingInterval: cdk.Duration.seconds(60),\n  processor: lambdaProcessor,\n  dataOutputPrefix: `${ddbChangesPrefix}/`,\n})\n\nconst firehoseDeliveryStream = new firehose.DeliveryStream(this, &#39;Delivery Stream&#39;, {\n  deliveryStreamName: `${name}-firehose`,\n  sourceStream: stream,\n  destinations: [s3Destination],\n})\n```\n\nThe delivery of the data to the S3 bucket is buffered. Here are the default values.\n\n![firehose-buffer]({{ site.baseurl }}/img/2021-10-26-aws_example_ddb_analytics_cdk/firehose_buffer.png)\n\n## Glue crawler\n\nAthena needs a structured table for the SQL queries. The Glue crawler creates this from the data in the S3 bucket below the configured prefix.\n\nThe glue crawler isn&#39;t a L2 construct yet. So it needs a L1 construct. See [here](https://blog.phillipninan.com/a-no-nonsense-guide-to-aws-cloud-development-kit-cdk) more about L1 - L3.\n\nThere is already a [github issue](https://github.com/aws/aws-cdk/issues/8863) to create a L2 construct for the glue crawler.\n\n\n```typescript\nconst getSecurityConfiguration = new iam.PolicyDocument({\n      statements: [\n        new iam.PolicyStatement({\n          actions: [&#39;glue:GetSecurityConfiguration&#39;],\n          resources: [&#39;*&#39;]\n        })\n      ]\n    })\n\n  const roleCrawler = new iam.Role(this, &#39;role crawler&#39;, {\n    roleName: `${name}-crawler-role`,\n    assumedBy: new iam.ServicePrincipal(&#39;glue.amazonaws.com&#39;),\n    inlinePolicies: {\n      GetSecurityConfiguration: getSecurityConfiguration\n    }\n  })\n\n  const glueDb = new glue.Database(this, &#39;glue db&#39;, {\n    databaseName: `${name}-db`,\n  })\n\n  const glueSecurityOptions = new glue.SecurityConfiguration(this, &#39;glue security options&#39;, {\n    securityConfigurationName: `${name}-security-options`,\n    s3Encryption: {\n      mode: glue.S3EncryptionMode.KMS,\n    },\n  })\n\n  const crawler = new glue.CfnCrawler(this, &#39;crawler&#39;, {\n    name: `${name}-crawler`,\n    role: roleCrawler.roleArn,\n    targets: {\n      s3Targets: [\n        {\n          path: `s3://${firehoseBucket.bucketName}/${ddbChangesPrefix}`,\n        },\n      ],\n    },\n    databaseName: glueDb.databaseName,\n    crawlerSecurityConfiguration: glueSecurityOptions.securityConfigurationName,\n  })\n\n  // const glueCrawlerLogArn = `arn:aws:logs:${cdk.Stack.of(this).region}:${cdk.Stack.of(this).account}:log-group:/aws-glue/crawlers:log-stream:${crawler.name}`\n  const glueCrawlerLogArn = `arn:aws:logs:${cdk.Stack.of(this).region}:${cdk.Stack.of(this).account}:log-group:/aws-glue/crawlers*` //:log-stream:${crawler.name}`\n\n  const glueTableArn = `arn:aws:glue:${cdk.Stack.of(this).region}:${cdk.Stack.of(this).account}:table/${glueDb.databaseName}/*`\n\n  const glueCrawlerArn = `arn:aws:glue:${cdk.Stack.of(this).region}:${cdk.Stack.of(this).account}:crawler/${crawler.name}`\n\n  roleCrawler.addToPolicy(\n    new iam.PolicyStatement({\n      resources: [\n        glueCrawlerLogArn,\n        glueTableArn,\n        glueDb.catalogArn,\n        glueDb.databaseArn,\n        kmsKey.keyArn,\n        firehoseBucket.bucketArn,\n        `${firehoseBucket.bucketArn}/*`,\n        glueCrawlerArn,\n      ],\n      actions: [&#39;logs:*&#39;, &#39;glue:*&#39;, &#39;kms:*&#39;, &#39;S3:*&#39;],\n    })\n  )\n```\n\nFor test purposes, it&#39;s enough to run the crawler before any analysis. Scheduling is also possible.\n\n![glue-run-crawler]({{ site.baseurl }}/img/2021-10-26-aws_example_ddb_analytics_cdk/glue_run_crawler.png)\n\nThat creates this table, which is accessible by Athena.\n\n![glue-table]({{ site.baseurl }}/img/2021-10-26-aws_example_ddb_analytics_cdk/glue_table.png)\n\n## Athena\n\nFor Athena it needs an S3 bucket for the query results and, for better isolation to other projects, a workgroup.\n\n```typescript\nconst athenaQueryResults = new s3.Bucket(this, &#39;query-results&#39;, {\n      bucketName: `${name}-query-results`,\n      encryptionKey: kmsKey,\n    })\n\nnew athena.CfnWorkGroup(this, &#39;analytics-athena-workgroup&#39;, {\n  name: `${name}-workgroup`,\n  workGroupConfiguration: {\n    resultConfiguration: {\n      outputLocation: `s3://${athenaQueryResults.bucketName}`,\n      encryptionConfiguration: {\n        encryptionOption: &#39;SSE_KMS&#39;,\n        kmsKey: kmsKey.keyArn,\n      },\n    },\n  },\n})\n```\n\nHow to anylyze the data see also [here]({{ site.baseurl }}/aws/2021/08/27/aws_example_ddb_analytics/)\n\n## Cost Alert 💰\n\n⚠️ Don&#39;t forget to destroy after testing. Kinesis Data Streams has [costs](https://aws.amazon.com/kinesis/data-streams/pricing/) per hour\n\n\n## Code\n\n[https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk](https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk)\n\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;Example how to analyze DynamoDB item changes with Kinesis and Athena created with CDK&quot;],&quot;summary&quot;:[0,&quot;This post is how stream data changes of a DynamoDb table via Kinesis Data Stream and Kinesis Firehose to S3, and analyze the data with Athena. Build with CDK.&quot;],&quot;date&quot;:[3,&quot;2021-10-26T06:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;aws&quot;],[0,&quot;aws kinesis&quot;],[0,&quot;aws athena&quot;],[0,&quot;aws cdk&quot;]]]}],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;2022-08-22-aws_example_ddb_analytics_athena_saved_queries_cdk.md&quot;],&quot;slug&quot;:[0,&quot;2022-08-22-aws_example_ddb_analytics_athena_saved_queries_cdk&quot;],&quot;body&quot;:[0,&quot;In [this]({{ site.baseurl }}/aws/2021/08/27/aws_example_ddb_analytics/) post is a example of a Athena query to get the the current data of the DynamoDb table.\n\nThis post explains how to provide this query with CDK as a saved query in Athena to have the query stored \&quot;nearby\&quot; the editor and that the query fits to the current deployment regarding the naming of the DB and table.\nAnother advantage is to have the query under source control.\n\nThis is a extension of [this]({{ site.baseurl }}/aws/2021/10/26/aws_example_ddb_analytics_cdk/) post.\n\nThe saved queries are stored here in the console:\n\n![athena save queries]({{ site.baseurl }}/img/2022-08-22-aws_example_ddb_analytics_athena_saved_queries_cdk/athena_saved_queries.png)\n\n# The SQL command\n\nThis SQL command will be provided as a saved query in Athena. For easier maintenance, the command is in an extra [SQL file](https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk/blob/main/cdk/lib/saved-queries/ddb-state.sql).\nThe DB and the table name are placeholders in the file and will be replaced during the deployment process. That ensures that this SQL command refers to the correct resources, which are also deployed.\n\n```sql\nSELECT dynamodb.newimage.pk.s AS pk,\n        dynamodb.newimage.person.M.firstname.s AS firstname,\n        dynamodb.newimage.person.M.lastname.s AS lastname,\n        dynamodb.approximatecreationdatetime AS ts,\n        dynamodb.newimage,\n        *\nFROM \&quot;athenaDbName\&quot;.\&quot;athenaTableName\&quot; AS persons1\nWHERE (eventname = &#39;INSERT&#39;\n        OR eventname = &#39;MODIFY&#39;)\n        AND dynamodb.approximatecreationdatetime =\n    (SELECT MAX(dynamodb.approximatecreationdatetime)\n    FROM \&quot;athenaDbName\&quot;.\&quot;athenaTableName\&quot; AS persons2\n    WHERE persons2.dynamodb.newimage.pk.s = persons1.dynamodb.newimage.pk.s);\n```\n\n\n# Saved Query Construct\n\nLike the SQL file, the CDK definition of the saved query is in an [extra](https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk/blob/main/cdk/lib/saved-queries/saved-queries.ts)[ file](https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk/blob/main/cdk/lib/saved-queries/saved-queries.ts) as a CDK construct.\n\n\n```typescript\nimport { Construct } from &#39;constructs&#39;\nimport {\n  aws_athena as athenaCfn\n} from &#39;aws-cdk-lib&#39;\nimport * as glue from &#39;@aws-cdk/aws-glue-alpha&#39;\nimport { readFileSync } from &#39;fs&#39;;\nimport { join } from &#39;path&#39;;\n\nexport interface SavedQueriesProps {\n  glueDb: glue.IDatabase;\n  athenaTableName: string;\n  athenaWorkgroupName: string;\n}\n\nexport class SavedQueries extends Construct {\n  constructor(scope: Construct, id: string, props: SavedQueriesProps) {\n    super(scope, id)\n\n    const getSqlString = (file: string): string =&gt; {\n      let personsDdbStateSqlCommand = readFileSync(join(__dirname, `${file}`), &#39;utf-8&#39;).toString()\n      const athenaDbName = props.glueDb.databaseName\n      let athenaTableName = props.athenaTableName;\n      athenaTableName = athenaTableName.replace(/-/g, &#39;_&#39;)\n      personsDdbStateSqlCommand = personsDdbStateSqlCommand.replace(/athenaDbName/g, athenaDbName)\n      personsDdbStateSqlCommand = personsDdbStateSqlCommand.replace(/athenaTableName/g, athenaTableName)\n      return personsDdbStateSqlCommand\n    }\n\n    let queryString = getSqlString(&#39;ddb-state.sql&#39;)\n\n    // eslint-disable-next-line no-new\n    new athenaCfn.CfnNamedQuery(this, &#39;query-current-ddb-state&#39;, {\n      database: props.glueDb.databaseName,\n      queryString: queryString,\n      description: &#39;query the current state from the ddb table&#39;,\n      name: &#39;current-ddb-state&#39;,\n      workGroup: props.athenaWorkgroupName\n    })\n\n  }\n}\n```\n\nCurrently, there is no L2 Construct for a named query in CDK, so it is defined as a L1 CFN Resource (`CfnNamedQuery`).\n\nThis needs the query as a string. The function `getSqlString` converts and transforms the SQL file content to that needed string.\n\nSo that the placeholders are replaced with the resources, which are deployed.\n\nThe Athena table name will be created during the glue crawler process. The convention is that the table name configured prefix of the S3 bucket with an underscore (\&quot;_\&quot;) instead of dashes (\&quot;-\&quot;).\n\nThe database name and the workgroup name are passed from the stack to the construct.\n\n# Insert the saved query construct in the stack\n\nFor the deployment the saved queries construct will be inserted into the [stack](https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk/blob/main/cdk/lib/cdk-stack.ts#L211)\n\nThe needed information are passed as props to the construct.\n\nThe saved queries are depending on the workgroup. That has to be defined in CDK. Otherwise, the deployment will fail.\n\n```typescript\nconst savedQueries = new SavedQueries(this, &#39;saved-queries&#39;, {\n      glueDb: glueDb,\n      athenaTableName: firehoseBucketName,\n      athenaWorkgroupName: athenaWorkgroup.name,\n    })\n\n    savedQueries.node.addDependency(athenaWorkgroup);\n```\n\n\n# Result\nAfter the deployment, the new query is listed here and can be chosen for query in the editor.\n\n![athena save query deployed]({{ site.baseurl }}/img/2022-08-22-aws_example_ddb_analytics_athena_saved_queries_cdk/athena_saved_query_deployed.png)\n\n![athena save queries]({{ site.baseurl }}/img/2022-08-22-aws_example_ddb_analytics_athena_saved_queries_cdk/athena_saved_query_editor.png)\n\n# Code\n\n[https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk](https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk)\n\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;Example how to create Athena saved queries with CDK&quot;],&quot;summary&quot;:[0,&quot;This post is about how saved queries are created with CDK. This is useful to have important queries prepared for any users.&quot;],&quot;date&quot;:[3,&quot;2022-08-22T06:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;aws&quot;],[0,&quot;aws athena&quot;],[0,&quot;aws cdk&quot;]]]}],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;2022-09-17-aws_example_ddb_analytics_quicksight_cdk.md&quot;],&quot;slug&quot;:[0,&quot;2022-09-17-aws_example_ddb_analytics_quicksight_cdk&quot;],&quot;body&quot;:[0,&quot;\nThis post is about how to visualize the DynamoDb data changes with Quicksight. It&#39;s an extension of [this]({{ site.baseurl }}/aws/2021/08/27/aws_example_ddb_analytics_cdk/) post, which describes how to analyze the data with Athena.\nThe setting for creating the DynamoDb table and putting the data changes to a S3 bucket is the same. Instead of creating an Athena table of the data in the S3 bucket, this data is linked to a data source in Quicksight. \n\n## Quicksight activation and costs\n\nQuicksight needs to be activated before using. It&#39;s enough to use the standard edition for this scenario. The first 30 days are [free](https://docs.aws.amazon.com/quicksight/latest/user/signing-up.html). Costs are listed [here](https://aws.amazon.com/quicksight/pricing/). \n\n![quicksight open service]({{ site.baseurl }}/img/2022-09-17-aws_example_ddb_analytics_quicksight_cdk/quicksight-open-service.png)\n\n![quicksight sign up]({{ site.baseurl }}/img/2022-09-17-aws_example_ddb_analytics_quicksight_cdk/quicksight-sign-up.png)\n\n![quicksight choose standard]({{ site.baseurl }}/img/2022-09-17-aws_example_ddb_analytics_quicksight_cdk/quicksight-choose-standard.png)\n\nEnter a name and an email address\n\n![quicksight create standard]({{ site.baseurl }}/img/2022-09-17-aws_example_ddb_analytics_quicksight_cdk/quicksight-create-standard.png)\n\n## Quicksight role\n\nIn the [standard edition](https://docs.aws.amazon.com/quicksight/latest/user/security_iam_service-with-iam.html#security-create-iam-role), Quicksight uses a standard role that could be configured via the Quicksight console.\n\n![quicksight permission access to aws services]({{ site.baseurl }}/img/2022-09-17-aws_example_ddb_analytics_quicksight_cdk/quicksight-permission-access-to-aws-services.png)\n\nUnfortunately, it is not possible to allow for specific KMS keys. For that, we need to add a policy to the role aws-quicksight-service-role-v0.\n\n[This](https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk/blob/main/cdk/lib/quicksight/quicksight-role.ts) ads the needed permissions to that role.\n\n```typescript\nimport { Construct } from &#39;constructs&#39;\nimport { aws_iam as iam, aws_s3 as s3 } from &#39;aws-cdk-lib&#39;\n\nexport interface QuicksightRoleProps {\n  name: string\n  bucket: s3.IBucket\n}\n\nexport class QuicksightRole extends Construct {\n  constructor(scope: Construct, id: string, props: QuicksightRoleProps) {\n    super(scope, id)\n\n    const quicksightRoleName = &#39;aws-quicksight-service-role-v0&#39;\n\n    const quicksightRole = iam.Role.fromRoleName(this, &#39;quicksight-role&#39;, quicksightRoleName)\n\n    quicksightRole.attachInlinePolicy(\n      new iam.Policy(this, `${props.name}-policy`, {\n        statements: [\n          new iam.PolicyStatement({\n            actions: [&#39;kms:Decrypt&#39;, &#39;s3:GetObject&#39;, &#39;s3:List*&#39;],\n            resources: [props.bucket.bucketArn, `${props.bucket.bucketArn}/*`, props.bucket.encryptionKey!.keyArn],\n          }),\n        ],\n      })\n    )\n  }\n}\n```\n\nNow it&#39;s possible to create a datasource from the S3 bucket.\n\n\n## Create a datasource and dataset\n\nIn Quicksight a data source is the connection to the data and the data set is using this data source and defining how the data will be used.\n\nQuicksight has a lot of different data sources. We want to use the data from S3.\n\n![quicksight datasource kinds]({{ site.baseurl }}/img/2022-09-17-aws_example_ddb_analytics_quicksight_cdk/quicksight-datasource-kinds.png)\n\nCurrently, there is no L2 CDK construct for data sources so we need to use the L1 cloud formation.\n\nIn the case of a S3 data source, it&#39;s a link to a manifest file\n\n```typescript\nconst datasource = new quicksightCfn.CfnDataSource(this, &#39;datasource&#39;, {\n      name: datasourceName,\n      type: &#39;S3&#39;,\n      awsAccountId: Stack.of(this).account,\n      dataSourceId: datasourceName,\n      dataSourceParameters: {\n        s3Parameters: {\n          manifestFileLocation: {\n            bucket: props.bucket.bucketName,\n            key: manifestKey,\n          },\n        },\n      },\n      permissions: permissionsDatasource,\n    })\n```\nThis is the definition of the manifest file. More about manifest files [here](https://docs.aws.amazon.com/quicksight/latest/user/supported-manifest-file-format.html)\n\n```typescript\nconst manifest = {\n      fileLocations: [\n        {\n          URIPrefixes: [`s3://${props.bucket.bucketName}/${props.prefix}/`],\n        },\n      ],\n      globalUploadSettings: {\n        format: &#39;JSON&#39;,\n      },\n    }\n```\n\n\nThe dataset then defines which fields will be used and has the potential to format these fields.\n\n```typescript\nconst dataset = new quicksightCfn.CfnDataSet(this, &#39;dataset&#39;, {\n      name: datasetName,\n      awsAccountId: Stack.of(this).account,\n      dataSetId: datasetName,\n      importMode: &#39;SPICE&#39;,\n      physicalTableMap: {\n        itemChanges: {\n          s3Source: {\n            dataSourceArn: datasource.attrArn,\n            uploadSettings: {\n              format: &#39;JSON&#39;,\n            },\n            inputColumns: [\n              {\n                name: &#39;awsRegion&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;eventID&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;eventName&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;userIdentity&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;recordFormat&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;tableName&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;dynamodb.ApproximateCreationDateTime&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;dynamodb.Keys.pk.S&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;dynamodb.NewImage.pk.S&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;dynamodb.NewImage.person.M.jobArea.S&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;dynamodb.NewImage.person.M.firstname.S&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;dynamodb.NewImage.person.M.gender.S&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;dynamodb.NewImage.person.M.jobType.S&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;dynamodb.NewImage.person.M.jobDescriptor.S&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;dynamodb.NewImage.person.M.lastname.S&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;dynamodb.SizeBytes&#39;,\n                type: &#39;STRING&#39;,\n              },\n              {\n                name: &#39;eventSource&#39;,\n                type: &#39;STRING&#39;,\n              },\n            ],\n          },\n        },\n      },\n      logicalTableMap: {\n        logicalTableProperty: {\n          alias: `${datasetName}-alias`,\n          source: { physicalTableId: &#39;itemChanges&#39; },\n        },\n      },\n      permissions: permissionsDataset,\n    });\n```\n\nThe whole definition is [here](https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk/blob/main/cdk/lib/quicksight/quicksight.ts)\n\n[Here](https://aws-blog.com/2021/09/building-quicksight-datasets-with-cdk-s3.html) is a post that describes it with Python CDK.\n\n\n### permission to see the datasource and dataset\n\nDatasources and datasets will be only displayed if your user has permission for that. That is not automatically the case if you deploy it with CDK.\nTherefore you need to put your user ARN to the permission. One way to do that is with an environment variable\n\n`QUICKSIGHT_USERNAME=&lt;&lt;Quicksight user name&gt;&gt; npx cdk deploy`\n\n\n```typescript\nconst quicksightUsername = process.env.QUICKSIGHT_USERNAME\n    const principalArn = `arn:aws:quicksight:${Stack.of(this).region}:${Stack.of(this).account}:user/default/${quicksightUsername}`\n\n    const permissionsDatasource = [\n      {\n        principal: principalArn,\n        actions: [\n          &#39;quicksight:DescribeDataSource&#39;,\n          &#39;quicksight:DescribeDataSourcePermissions&#39;,\n          &#39;quicksight:PassDataSource&#39;,\n          &#39;quicksight:UpdateDataSource&#39;,\n          &#39;quicksight:DeleteDataSource&#39;,\n          &#39;quicksight:UpdateDataSourcePermissions&#39;,\n        ],\n      },\n    ]\n\n    const permissionsDataset = [\n      {\n        principal: principalArn,\n        actions: [\n          &#39;quicksight:DescribeDataSet&#39;,\n          &#39;quicksight:DescribeDataSetPermissions&#39;,\n          &#39;quicksight:PassDataSet&#39;,\n          &#39;quicksight:DescribeIngestion&#39;,\n          &#39;quicksight:ListIngestions&#39;,\n          &#39;quicksight:UpdateDataSet&#39;,\n          &#39;quicksight:DeleteDataSet&#39;,\n          &#39;quicksight:CreateIngestion&#39;,\n          &#39;quicksight:CancelIngestion&#39;,\n          &#39;quicksight:UpdateDataSetPermissions&#39;,\n        ],\n      },\n    ]\n```\n\nYou can find the Quicksight username here\n\n![quicksight username]({{ site.baseurl }}/img/2022-09-17-aws_example_ddb_analytics_quicksight_cdk/quicksight-username.png)\n\n### Deployment and refresh of the dataset\n\nIf you deploy this setup the first time, the data does not exist, but the datasource links already to the data. That causes a deployment error.\nTo avoid this a little dummy data file will be deployed with a SDK call of a CDK custom resource\n\n```typescript\nconst dummyJsonString = JSON.stringify({ dummy: &#39;dummy&#39;}); // Delete after deplyoment\n    const customResourcePutObject = new custom_resources.AwsCustomResource(this, &#39;prefix-creation&#39;, { // add -put\n      onCreate: {\n        service: &#39;S3&#39;,\n        action: &#39;putObject&#39;,\n        parameters: {\n          Bucket: props.bucket.bucketName,\n          Key: `${props.prefix}/dummy.json`,\n          Body: dummyJsonString,\n        },\n        physicalResourceId: custom_resources.PhysicalResourceId.of(&#39;prefix-creation&#39;),\n      },\n      policy: custom_resources.AwsCustomResourcePolicy.fromSdkCalls({ resources: custom_resources.AwsCustomResourcePolicy.ANY_RESOURCE }),\n    });\n    props.\n```\n\nAfter the datasource is deployed this will be removed\n\n```typescript\nconst customResourceDeleteObject = new custom_resources.AwsCustomResource(this, &#39;prefix-creation-delete&#39;, {\n      onCreate: {\n        service: &#39;S3&#39;,\n        action: &#39;deleteObject&#39;,\n        parameters: {\n          Bucket: props.bucket.bucketName,\n          Key: `${props.prefix}/dummy.json`,\n        },\n        physicalResourceId: custom_resources.PhysicalResourceId.of(&#39;prefix-creation&#39;),\n      },\n      policy: custom_resources.AwsCustomResourcePolicy.fromSdkCalls({ resources: custom_resources.AwsCustomResourcePolicy.ANY_RESOURCE }),\n    });\n    props.bucket.grantReadWrite(customResourceDeleteObject);\n    customResourceDeleteObject.node.addDependency(dataset);\n```\n\n\nAfter there is some data in the dynamodb you have to refresh the dataset. This is how it looks like if you create 5 new entries and then modify 1 and refresh again and use this data in an analysis.\n\n![quicksight refresh dataset]({{ site.baseurl }}/img/2022-09-17-aws_example_ddb_analytics_quicksight_cdk/quicksight-refresh-dataset.png)\n\n![quicksight dataset imported rows]({{ site.baseurl }}/img/2022-09-17-aws_example_ddb_analytics_quicksight_cdk/quicksight-dataset-imported-rows.png)\n\n![quicksight dataset analysis]({{ site.baseurl }}/img/2022-09-17-aws_example_ddb_analytics_quicksight_cdk/quicksight-dataset-analysis.png)\n\n![quicksight analysis inserts]({{ site.baseurl }}/img/2022-09-17-aws_example_ddb_analytics_quicksight_cdk/quicksight-analysis-inserts.png)\n\n![quicksight analysis modify]({{ site.baseurl }}/img/2022-09-17-aws_example_ddb_analytics_quicksight_cdk/quicksight-analysis-modify.png)\n\n\n\n# Cost Alert 💰\n\n⚠️ Don&#39;t forget to delete the Quicksight account after testing.\n\n![quicksight delete account]({{ site.baseurl }}/img/2022-09-17-aws_example_ddb_analytics_quicksight_cdk/quicksight-delete-account.png)\n\n\n# Code\n\n[https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk](https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk)\n\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;Example how to visualize DynamoDB item changes with Quicksight (S3 source) created with CDK&quot;],&quot;summary&quot;:[0,&quot;This post is how to visualize the streamed data changes of a DynamoDb table via Kinesis Data Stream and Kinesis Firehose to S3. Build with CDK.&quot;],&quot;date&quot;:[3,&quot;2022-09-17T06:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;aws&quot;],[0,&quot;aws kinesis&quot;],[0,&quot;aws athena&quot;],[0,&quot;aws cdk&quot;],[0,&quot;aws quicksight&quot;]]]}],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;2022-10-03-aws_example_ddb_export_athena_query.md&quot;],&quot;slug&quot;:[0,&quot;2022-10-03-aws_example_ddb_export_athena_query&quot;],&quot;body&quot;:[0,&quot;\nIn [this]({{ site.baseurl }}/aws/2021/08/27/aws_example_ddb_analytics_cdk/) post is described how to get the data to analyze the changes in the dynamodb data. This post describes how to (semi) automate the export of the dynamodb table data and analyze it with Athena. [This](https://aws.amazon.com/de/blogs/aws/new-export-amazon-dynamodb-table-data-to-data-lake-amazon-s3/) post describes how you can do that manually. \n\nOne approach is with a lambda and another approach is with step functions. Both approaches implement the steps for triggering the export to a S3 bucket, create an athena table for that exported data and prepare a namend query for analyzing.\n\nThe data for this example looks like this.\n\n![ddb export ddb data]({{ site.baseurl }}/img/2022-10-03-aws_example_ddb_export_athena_query/ddb-export-ddb-data.png)\n\n\n## With lambda\n\n[This lambda](https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk/blob/main/cdk/lib/ddb-export/ddb-export.lambda-function-ddb-export.ts) triggers the export with via the sdk and create or update a named query.\n\n[The query](https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk/blob/main/cdk/lib/ddb-export/createTable.sql) creates the athena table. The export id will be set by the lambda by replacing the \&quot;s3location\&quot; with something like `s3://&lt;&lt;bucket name&gt;&gt;/ddb-exports/AWSDynamoDB/&lt;&lt;ddb-export-id&gt;&gt;/data/`.\n\n```SQL\nCREATE EXTERNAL TABLE ddb_exported_table (\n Item struct&lt;pk:struct&lt;S:string&gt;,\n             person:struct&lt;M:struct&lt;\n                jobArea:struct&lt;S:string&gt;,\n                firstname:struct&lt;S:string&gt;,\n                gender:struct&lt;S:string&gt;,\n                jobType:struct&lt;S:string&gt;,\n                jobDescriptor:struct&lt;S:string&gt;,\n                lastname:struct&lt;S:string&gt;\n                &gt;&gt;&gt;\n)\nROW FORMAT SERDE &#39;org.openx.data.jsonserde.JsonSerDe&#39;\nLOCATION &#39;s3Location&#39;\nTBLPROPERTIES ( &#39;has_encrypted_data&#39;=&#39;true&#39;);\n```\n\nhttps://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk/blob/main/cdk/lib/ddb-export/readTable.sql\n\n```SQL\nSELECT \nitem.pk.S as pk,\nitem.person.M.firstname.S as firstname,\nitem.person.M.lastname.S as lastname,\nitem.person.M.jobArea.S as jobArea,\nitem.person.M.gender.S as gender, \nitem.person.M.jobType.S as jobType, \nitem.person.M.jobDescriptor.S as jobDescriptor\nFROM \&quot;db_name\&quot;.\&quot;table_name\&quot;;\n```\n\nAfter you started the lambda you have to wait until the export is finished. Then you can run the query for creating the athena table. The lambda has already deleted the old table. After that you can use the prepared query for analyzing.\n\nA more orchestrated approach is with step function. That&#39;s better for waiting for the results :)\n\n## With step functions\n\nThis are the steps, which are orchestrated by the step function. \n\n![ddb export sfn]({{ site.baseurl }}/img/2022-10-03-aws_example_ddb_export_athena_query/ddb-export-sfn.png)\n\nIt&#39;s definend [here](https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk/blob/main/cdk/lib/ddb-export/ddb-export-step-function.ts)\n\nThe step function could be startet with the default values.\n\n![ddb export sfn start 1]({{ site.baseurl }}/img/2022-10-03-aws_example_ddb_export_athena_query/ddb-export-sfn-start-1.png)\n\n![ddb export sfn start 2]({{ site.baseurl }}/img/2022-10-03-aws_example_ddb_export_athena_query/ddb-export-sfn-start-2.png)\n\nIt takes some minutes to complete.\n\n![ddb export sfn run]({{ site.baseurl }}/img/2022-10-03-aws_example_ddb_export_athena_query/ddb-export-sfn-run.png)\n\nThe \&quot;recent queries\&quot; section list the steps for dropping the old table and create the new one.\n\n![ddb export sfn athena recent queries]({{ site.baseurl }}/img/2022-10-03-aws_example_ddb_export_athena_query/ddb-export-sfn-athena-recent-queries.png)\n\nAfter it&#39;s finished you can choose the saved query with the name `sfn-ddb-export-read-table`. It can be used to query all the data from the dynamodb table and could be adapted to more \&quot;complex\&quot; queries.\n\n![ddb export sfn athena query]({{ site.baseurl }}/img/2022-10-03-aws_example_ddb_export_athena_query/ddb-export-sfn-athena-query.png)\n\n\n## Code\n\n[https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk](https://github.com/JohannesKonings/test-aws-dynamodb-athena-cdk)\n\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;Example how to trigger a Dynamodb export and create an Athena saved query with CDK&quot;],&quot;summary&quot;:[0,&quot;This post is how to trigger a Dynamodb export and create saved query to create a Athena table from the exported data&quot;],&quot;date&quot;:[3,&quot;2022-10-03T06:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;aws&quot;],[0,&quot;aws athena&quot;],[0,&quot;aws cdk&quot;],[0,&quot;aws dynamodb&quot;]]]}],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;2023-08-19-cdk-serverless-v2-demo-with-zod.md&quot;],&quot;slug&quot;:[0,&quot;2023-08-19-cdk-serverless-v2-demo-with-zod&quot;],&quot;body&quot;:[0,&quot;\r\nThe [AWS CDK Serverless Toolsuite](https://github.com/taimos/cdk-serverless) from [Thorsten Hoeger](https://github.com/hoegertn) helps, among others, to deploy an API Gateway from [OpenApi specs](https://www.openapis.org/) and a DynamoDb from [DynamoDb onetable data modeling](https://doc.onetable.io/). The advantage is to leverage the type safety from Typscript generated from these files.\r\n\r\nThat helps during the development cycle, but a runtime Typescript is Javascript without any type checking. This post is about enhancing this setup with [zod](https://github.com/colinhacks/zod) to validate the types during runtime.\r\n\r\nThe workflow so far is to create a definition and generate Typescript types from that. Now the workflow has a step before to create a zod schema and derive the definitions from the zod schema.\r\n\r\n## Type checking without zod\r\n\r\nAs you can see, the types are available at development time via the OpenApi spec.\r\n\r\n![add Todo title type]({{ site.baseurl }}/img/2023-08-19-cdk-serverless-v2-demo-with-zod/addTodoTitleType.png)\r\n\r\nThis is because of the defined components in this [openApi spec](https://github.com/JohannesKonings/cdk-serverless-v2-demo/blob/main/src/definitions/myapi.yaml)\r\n\r\n```yaml\r\ncomponents:\r\n  schemas:\r\n    Todo:\r\n      type: object\r\n      required:\r\n        - id\r\n        - state\r\n        - title\r\n        - description\r\n        - lastUpdate\r\n      properties:\r\n        id: \r\n          type: string\r\n        state: \r\n          type: string\r\n        title: \r\n          type: string\r\n        description: \r\n          type: string\r\n        lastUpdate:\r\n          type: string\r\n          format: date-time\r\n    AddTodo:\r\n      type: object\r\n      required:\r\n        - title\r\n        - description\r\n      properties:\r\n        title: \r\n          type: string\r\n        description: \r\n          type: string\r\n```\r\n\r\nBut that didn&#39;t prevent you from using the false type during runtime.\r\n\r\n![add Todo title as number]({{ site.baseurl }}/img/2023-08-19-cdk-serverless-v2-demo-with-zod/addTodoTitleAsNumber.png)\r\n\r\n## Type checking with zod\r\n\r\nWith a one-line parsing command, zod checks all the types.\r\n\r\n![add Todo zod parsing]({{ site.baseurl }}/img/2023-08-19-cdk-serverless-v2-demo-with-zod/addTodoZodParsing.png)\r\n\r\nFurthermore, zod has some [string-specific validations](https://github.com/colinhacks/zod#strings) that can check if the email is valid.\r\n\r\n```typescript\r\nnotificationsEmail: z.string().email(),\r\n```\r\n![add Todo validation result]({{ site.baseurl }}/img/2023-08-19-cdk-serverless-v2-demo-with-zod/addTodoValidationResult.png)\r\n\r\n\r\n## Implementation\r\n\r\nTo implement that, first, the zod schemas are needed. [This setup](https://github.com/JohannesKonings/cdk-serverless-v2-demo/blob/main/src/zod/schema-todo.ts) has three schemas.\r\nOne for the API request, one for the API response and one how the data is stored.\r\n\r\nWith zod it&#39;s possible to reference existing schema an extend fields or omit some.\r\n\r\n```typescript\r\nimport * as z from &#39;zod&#39;;\r\n\r\nexport const schemaTodoApi = z.object({\r\n  id: z.string().uuid(),\r\n  state: z.enum([&#39;OPEN&#39;, &#39;IN PROGRESS&#39;, &#39;DONE&#39;]).default(&#39;OPEN&#39;),\r\n  title: z.string(),\r\n  finishedInDays: z.number().int().positive(),\r\n  notificationsEmail: z.string().email(),\r\n  description: z.string().optional(),\r\n  lastUpdate: z.string().datetime(),\r\n});\r\n\r\nexport const schemaAddTodoApi = schemaTodoApi.omit({\r\n  id: true,\r\n  state: true,\r\n  lastUpdate: true,\r\n});\r\n\r\nexport const schemaTodoDdb = schemaTodoApi.extend({\r\n  lastUpdated: z.string().datetime(),\r\n}).omit({ lastUpdate: true });\r\n```\r\n\r\n### openApi\r\n\r\nTo create the openApi spec I&#39;m using the `@asteasolutions/zod-to-openapi` package. zod has listed some more packages [here](https://github.com/colinhacks/zod#zod-to-x) which can be used.\r\n\r\nThe definition of the apiSpec is now created via Typescript as you can see [here](https://github.com/JohannesKonings/cdk-serverless-v2-demo/blob/main/src/zod/openapi.ts) and generate a yaml file.\r\n\r\n```typescript\r\nimport fs from &#39;node:fs&#39;;\r\nimport {\r\n  extendZodWithOpenApi,\r\n  OpenAPIRegistry,\r\n  OpenApiGeneratorV3,\r\n} from &#39;@asteasolutions/zod-to-openapi&#39;;\r\nimport yaml from &#39;js-yaml&#39;;\r\nimport * as z from &#39;zod&#39;;\r\nimport { schemaAddTodoApi, schemaTodoApi } from &#39;./schema-todo&#39;;\r\n\r\nextendZodWithOpenApi(z);\r\n\r\nconst registry = new OpenAPIRegistry();\r\n\r\nconst apiKeyComponent = registry.registerComponent(\r\n  &#39;securitySchemes&#39;,\r\n  &#39;api_key&#39;,\r\n  {\r\n    type: &#39;apiKey&#39;,\r\n    name: &#39;x-api-key&#39;,\r\n    in: &#39;header&#39;,\r\n  },\r\n);\r\n\r\nregistry.register(\r\n  &#39;Todo&#39;,\r\n  schemaTodoApi.openapi({}),\r\n);\r\nregistry.register(\r\n  &#39;AddTodo&#39;,\r\n  schemaAddTodoApi.openapi({}),\r\n);\r\n\r\nregistry.registerPath({\r\n  method: &#39;get&#39;,\r\n  path: &#39;/todos&#39;,\r\n  summary: &#39;return list of todos&#39;,\r\n  tags: [&#39;admin&#39;],\r\n  security: [{ [apiKeyComponent.name]: [] }],\r\n  operationId: &#39;getTodos&#39;,\r\n  responses: {\r\n    200: {\r\n      description: &#39;successful operation&#39;,\r\n      content: {\r\n        &#39;application/json&#39;: {\r\n          schema: {\r\n            type: &#39;array&#39;,\r\n            items: {\r\n              $ref: &#39;#/components/schemas/Todo&#39;,\r\n            },\r\n          },\r\n        },\r\n        &#39;text/calendar&#39;: {\r\n          schema: {\r\n            type: &#39;string&#39;,\r\n          },\r\n        },\r\n      },\r\n    },\r\n  },\r\n});\r\nregistry.registerPath({\r\n  &#39;method&#39;: &#39;post&#39;,\r\n  &#39;path&#39;: &#39;/todos&#39;,\r\n  &#39;summary&#39;: &#39;add new todo&#39;,\r\n  &#39;tags&#39;: [&#39;admin&#39;],\r\n  &#39;security&#39;: [{ [apiKeyComponent.name]: [] }],\r\n  &#39;operationId&#39;: &#39;addTodo&#39;,\r\n  &#39;requestBody&#39;: {\r\n    required: true,\r\n    content: {\r\n      &#39;application/json&#39;: {\r\n        schema: {\r\n          $ref: &#39;#/components/schemas/AddTodo&#39;,\r\n        },\r\n      },\r\n    },\r\n  },\r\n  &#39;responses&#39;: {\r\n    201: {\r\n      description: &#39;successful operation&#39;,\r\n      content: {\r\n        &#39;application/json&#39;: {\r\n          schema: {\r\n            $ref: &#39;#/components/schemas/Todo&#39;,\r\n          },\r\n        },\r\n      },\r\n    },\r\n    401: {\r\n      description: &#39;you are not logged in&#39;,\r\n      content: {},\r\n    },\r\n    403: {\r\n      description: &#39;you are not authorized to add todos&#39;,\r\n      content: {},\r\n    },\r\n  },\r\n  &#39;x-codegen-request-body-name&#39;: &#39;body&#39;,\r\n});\r\nregistry.registerPath({\r\n  method: &#39;post&#39;,\r\n  path: &#39;/todos/{id}&#39;,\r\n  summary: &#39;get a todo by its id&#39;,\r\n  tags: [&#39;admin&#39;],\r\n  security: [{ [apiKeyComponent.name]: [] }],\r\n  operationId: &#39;getTodoById&#39;,\r\n  responses: {\r\n    200: {\r\n      description: &#39;successful operation&#39;,\r\n      content: {\r\n        &#39;application/json&#39;: {\r\n          schema: {\r\n            $ref: &#39;#/components/schemas/Todo&#39;,\r\n          },\r\n        },\r\n      },\r\n    },\r\n    401: {\r\n      description: &#39;you are not logged in&#39;,\r\n      content: {},\r\n    },\r\n    403: {\r\n      description: &#39;you are not authorized to add todos&#39;,\r\n      content: {},\r\n    },\r\n  },\r\n});\r\nregistry.registerPath({\r\n  method: &#39;delete&#39;,\r\n  path: &#39;/todos/{id}&#39;,\r\n  summary: &#39;delete a todo&#39;,\r\n  tags: [&#39;admin&#39;],\r\n  security: [{ [apiKeyComponent.name]: [] }],\r\n  operationId: &#39;removeTodo&#39;,\r\n  responses: {\r\n    200: {\r\n      description: &#39;successful operation&#39;,\r\n      content: {},\r\n    },\r\n  },\r\n});\r\n\r\nconst generator = new OpenApiGeneratorV3(registry.definitions);\r\n\r\nconst generatorDocument = generator.generateDocument({\r\n  openapi: &#39;3.0.1&#39;,\r\n  info: {\r\n    version: &#39;1.0&#39;,\r\n    title: &#39;Serverless Demo with zod&#39;,\r\n  },\r\n  tags: [\r\n    {\r\n      name: &#39;info&#39;,\r\n    },\r\n    {\r\n      name: &#39;admin&#39;,\r\n    },\r\n  ],\r\n});\r\n\r\nconst yamlString = yaml.dump(generatorDocument, { indent: 2 });\r\n\r\nfs.writeFileSync(&#39;./src/definitions/myapi-zod.yaml&#39;, yamlString);\r\n```\r\n\r\n### onetable\r\n\r\nUnfortunately, for onetable didn&#39;t exist a npm package. So the conversion is made from scratch.\r\n[This](https://github.com/JohannesKonings/cdk-serverless-v2-demo/blob/main/src/zod/onetable.ts) is how it looks like.\r\n\r\n```typescript\r\nimport fs from &#39;node:fs&#39;;\r\nimport { z } from &#39;zod&#39;;\r\nimport { schemaTodoDdb } from &#39;./schema-todo&#39;;\r\n\r\nconst modelTodoDdb = {\r\n  PK: {\r\n    type: &#39;string&#39;,\r\n    value: &#39;TODO#${id}&#39;,\r\n  },\r\n  SK: {\r\n    type: &#39;string&#39;,\r\n    value: &#39;TODO#${id}&#39;,\r\n  },\r\n  id: {\r\n    type: &#39;string&#39;,\r\n    required: true,\r\n    generate: &#39;uuid&#39;,\r\n  },\r\n  GSI1PK: {\r\n    type: &#39;string&#39;,\r\n    value: &#39;TODOS&#39;,\r\n  },\r\n  GSI1SK: {\r\n    type: &#39;string&#39;,\r\n    value: &#39;${state}#${title}&#39;,\r\n  },\r\n};\r\n\r\nconst schemaTodoValues = schemaTodoDdb.keyof().Values;\r\n\r\nconst modelTodoFields = Object.keys(schemaTodoValues).reduce((acc, key) =&gt; {\r\n  const keyOfSchemaTodoKeyValues = key as keyof typeof schemaTodoValues;\r\n  const shapeType = schemaTodoDdb.shape[keyOfSchemaTodoKeyValues];\r\n\r\n  const { type, required, generate, enumValues, defaultValue } = deriveAttributes(shapeType);\r\n\r\n  return {\r\n    ...acc,\r\n    [key]: {\r\n      type: type,\r\n      required: required,\r\n      generate: generate,\r\n      enum: enumValues,\r\n      default: defaultValue,\r\n    },\r\n  };\r\n}, {});\r\n\r\nfunction deriveAttributes(shapeType: z.ZodType&lt;any, any&gt;) {\r\n  let type = &#39;&#39;;\r\n  let required = false;\r\n  let generate = undefined;\r\n  let enumValues = [] as string[];\r\n  let defaultValue = undefined;\r\n\r\n  if (shapeType === undefined) {\r\n    throw new Error(&#39;type is undefined&#39;);\r\n  } else if (shapeType instanceof z.ZodString) {\r\n    type = &#39;string&#39;;\r\n    required = true;\r\n    generate = shapeType.isUUID ? &#39;uuid&#39; : undefined;\r\n  } else if (shapeType instanceof z.ZodNumber) {\r\n    type = &#39;number&#39;;\r\n    required = true;\r\n  } else if (shapeType instanceof z.ZodEnum) {\r\n    required = true;\r\n    type = &#39;string&#39;;\r\n    enumValues = shapeType._def.values;\r\n  } else if (shapeType instanceof z.ZodDefault) {\r\n    required = true;\r\n    defaultValue = shapeType._def.defaultValue();\r\n    const { type: typeInnerType, enumValues: enumInnerType } = deriveAttributes(shapeType._def.innerType);\r\n    type = typeInnerType;\r\n    enumValues = enumInnerType as string[];\r\n  } else if (shapeType instanceof z.ZodOptional) {\r\n    required = false;\r\n    const { type: typeInnerType } = deriveAttributes(shapeType._def.innerType);\r\n    type = typeInnerType;\r\n  } else {\r\n    console.log(&#39;shapeType&#39;, shapeType);\r\n    throw new Error(&#39;type is not supported&#39;);\r\n  }\r\n  return {\r\n    type,\r\n    required: required ? true : undefined,\r\n    generate,\r\n    enumValues: enumValues &amp;&amp; enumValues.length &gt; 0 ? enumValues : undefined,\r\n    defaultValue,\r\n  };\r\n}\r\n\r\nexport const modelTodo = {\r\n  ...modelTodoDdb,\r\n  ...modelTodoFields,\r\n};\r\n\r\nconst onetable = {\r\n  indexes: {\r\n    primary: {\r\n      hash: &#39;PK&#39;,\r\n      sort: &#39;SK&#39;,\r\n    },\r\n    GSI1: {\r\n      hash: &#39;GSI1PK&#39;,\r\n      sort: &#39;GSI1SK&#39;,\r\n      project: &#39;all&#39;,\r\n    },\r\n    LSI1: {\r\n      type: &#39;local&#39;,\r\n      sort: &#39;lastUpdated&#39;,\r\n      project: [\r\n        &#39;id&#39;,\r\n        &#39;lastUpdated&#39;,\r\n        &#39;title&#39;,\r\n      ],\r\n    },\r\n  },\r\n  models: {\r\n    Todo: modelTodo,\r\n  },\r\n  version: &#39;0.1.0&#39;,\r\n  format: &#39;onetable:1.1.0&#39;,\r\n  queries: {},\r\n};\r\n\r\nfs.writeFileSync(&#39;./src/definitions/mymodel-zod.json&#39;, JSON.stringify(onetable, null, 2));\r\n```\r\n\r\n### Integration into the file creation workflow\r\n\r\nThe definition files can now be generated based on a zod schema. So that that will happen together with generating the files from the spec the projen.ts file need to be enhanced. This will create two commands before.\r\n\r\n```typescript\r\nconst taskDefinitionsCreation = project.addTask(&#39;definitionsCreation&#39;, {\r\n  steps: [\r\n    { exec: &#39;ts-node ./src/zod/openapi.ts&#39; },\r\n    { exec: &#39;ts-node ./src/zod/onetable.ts&#39; },\r\n  ],\r\n});\r\nproject.defaultTask!.prependSpawn(taskDefinitionsCreation);\r\n```\r\n\r\nThan the steps look like this.\r\n\r\n```JSON\r\n  \&quot;default\&quot;: {\r\n      \&quot;name\&quot;: \&quot;default\&quot;,\r\n      \&quot;description\&quot;: \&quot;Synthesize project files\&quot;,\r\n      \&quot;steps\&quot;: [\r\n        {\r\n          \&quot;spawn\&quot;: \&quot;definitionsCreation\&quot;\r\n        },\r\n        {\r\n          \&quot;exec\&quot;: \&quot;ts-node --project tsconfig.dev.json .projenrc.ts\&quot;\r\n        },\r\n        {\r\n          \&quot;spawn\&quot;: \&quot;generate:api:myapi\&quot;\r\n        }\r\n      ]\r\n    },\r\n```\r\n\r\nNow with the command `npm run projen` the definition file are created derived from zod and from that on the workflow is like before.\r\n\r\n## Code\r\n\r\n[https://github.com/JohannesKonings/cdk-serverless-v2-demo](https://github.com/JohannesKonings/cdk-serverless-v2-demo)\r\n\r\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;Example how to use zod with CDK serverless v2&quot;],&quot;summary&quot;:[0,&quot;CDK serverless v2 is for using type saftey develepmonet base on schemas like openAPI. This post is a example how to use zod with CDK serverless v2&quot;],&quot;date&quot;:[3,&quot;2023-08-19T06:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;aws&quot;],[0,&quot;aws cdk&quot;],[0,&quot;projen&quot;],[0,&quot;zod&quot;]]]}],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;2023-09-08-aws-sso-steampipe.md&quot;],&quot;slug&quot;:[0,&quot;2023-09-08-aws-sso-steampipe&quot;],&quot;body&quot;:[0,&quot;\n## Use case\n\nYou are in charge of several AWS accounts within an AWS Organisation and need to check the resources across these accounts. E.g., to check which are the configured runtimes for the lambdas.\n\n## Approach with Steampipe\n\n[Steampipe](https://steampipe.io/) is a tool to query data from different providers. Among others, there is a [plugin for AWS](https://hub.steampipe.io/plugins/turbot/aws).\n\nThe big plus is that Steampipe provides the ability to query more than one account with a query with [aggregator connection](https://steampipe.io/docs/managing/connections#using-aggregators)\n\nThis is how the result will look like for my AWS SSO accounts.\n\n![query result]({{ site.baseurl }}/img/2023-09-08-aws-sso-steampipe/query-result.png)\n\nMore about Steampipe and AWS: https://dev.to/aws-builders/easily-query-your-cloud-inventory-with-steampipe-2af3\n\n## Setup\n\nIt&#39;s necessary to have a link between AWS CLI profiles and Steampipe connection for the AWS SSO accounts that can be recreated without any effects on the local setup, which is created inside a docker image. This [docker image](https://github.com/JohannesKonings/aws-sso-steampipe/blob/main/Dockerfile) is based on Steampipe with additional installation of some tools, the AWS CLI and the AWS steampipe plugin.\n\n### Docker\n\n```Dockerfile\nFROM ghcr.io/turbot/steampipe\n\n# Setup prerequisites (as root)\nUSER root:0\nRUN apt-get update -y \\\n &amp;&amp; apt-get install -y git curl unzip jq\n\nRUN curl \&quot;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\&quot; -o \&quot;awscliv2.zip\&quot; \\\n &amp;&amp; unzip awscliv2.zip \\\n &amp;&amp; ./aws/install \\\n &amp;&amp; rm -rf awscliv2.zip ./aws\n\n\n# Install the aws and steampipe plugins for Steampipe (as steampipe user).\nUSER steampipe:0\nRUN  steampipe plugin install steampipe aws\n```\nThe Steampipe docu is here: https://steampipe.io/docs/managing/containers\n\nAfter creating the image with `docker build -t steampipe-query .`. The container can be created with the following command.\n\n```bash\ndocker run --entrypoint /bin/bash -it \\\n--mount type=bind,source=\&quot;${PWD}/queries\&quot;,target=/workspace/queries \\\n--mount type=bind,source=\&quot;${PWD}/scripts\&quot;,target=/workspace/scripts \\\n--mount type=bind,source=\&quot;${PWD}/.env\&quot;,target=/workspace/.env \\\n--name steampipe-query \\\nsteampipe-query \n```\n\nThese are the commands to use the container again `docker start -a steampipe-query` and `docker exec -it steampipe-query /bin/bash`.\n\n## Queries\n\nOne of the mount points was the folder queries, which contain, in this example, the SQL to check the lambda runtime.\n\n```SQL\nselect\n  account_id,\n  _ctx -&gt;&gt; &#39;connection_name&#39; as connection_name,\n  runtime,\n  count(*),\n  SUM(COUNT(*)) OVER() AS total_count\nfrom\n  aws_all.aws_lambda_function\nwhere runtime not in (&#39;nodejs18.x&#39;, &#39;nodejs16.x&#39;, &#39;python3.9&#39;)\ngroup by\n  account_id,\n  _ctx,\n  runtime\norder by\n  connection_name,\n  runtime,\n  count;\n```\n\nThe command to run this query is `steampipe query queries/lambda-runtime.sql`. This will work after the scripts have created the profiles and connections config.\n\n## Scripts\n\nThe other mount points are scripts and the env file. The first step is to set the needed env variable values and then run the script `./scripts/create-aws-config.sh` inside the container, which creates the file ~/.aws/config with SS0 session values.\n\n```env\nSSO_START_URL= # https://&lt;your-aws-account-id&gt;.awsapps.com/start\nSSO_SESSION_NAME= # &lt;your session name, it&#39;s just a name&gt;\nSSO_REGION= # &lt;your region, e.g. us-east-1&gt;\n```\nAs next step source the env file with `source .env` to get the value for the session name. Than run the login to aws sso with the command `aws sso login --sso-session $SSO_SESSION_NAME`.\n\nIt will look like this.\n\n![sso login]({{ site.baseurl }}/img/2023-09-08-aws-sso-steampipe/sso-login.png)\n\nOpen the link in the browser and put in the code.\n\n![authorize request]({{ site.baseurl }}/img/2023-09-08-aws-sso-steampipe/authorize-request.png)\n\nThen, allow the access.\n\n![allow sso to access data]({{ site.baseurl }}/img/2023-09-08-aws-sso-steampipe/allow-sso-to-access-data.png)\n\n![successfully logged in]({{ site.baseurl }}/img/2023-09-08-aws-sso-steampipe/successfully-logged-in.png)\n\nAfter it&#39;s confirmed, you can create profiles with the script `./scripts/create-aws-profiles.sh` inside the container. This will create a profile for each account in the aws config file ~/.aws/config (after confirmation) with a suffix of the assigned roles for the accounts.\n\nThe scipt is adapted from this gist: https://gist.github.com/lukeplausin/3cfedc29755e184ef526b504c77ffe70\n\nThe last step for the setup is to create the connections for Steampipe with the script `./scripts/create-aws-connections.sh` inside the container. This will create a connection for each profile in the AWS config file ~/.aws/config.\nNot every role is allowed to query the data, so it&#39;s necessary to set the env variable `ALLOWED_ROLES` with the roles allowed to query the data. The roles are comma-separated. E.g. \n\n`ALLOWED_ROLES=\&quot;AWSReadOnlyAccess,AWSAdministratorAccess\&quot;`\n\nAnd now it&#39;s possible to run the queries with steampipe 🥳\n\n## Code\n\n[https://github.com/JohannesKonings/aws-sso-steampipe](https://github.com/JohannesKonings/aws-sso-steampipe)\n\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;Use Steampipe to select your AWS resources across SSO accounts with SQL&quot;],&quot;summary&quot;:[0,&quot;See how this setup can help to discover your AWS resources across all SSO accounts with a mix of Steampipe, docker, bash scripts, and AWS CLI&quot;],&quot;date&quot;:[3,&quot;2023-09-08T06:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;aws&quot;],[0,&quot;steampipe&quot;],[0,&quot;docker&quot;]]]}],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;2023-12-16-cdk-notifier-feature-stacks.md&quot;],&quot;slug&quot;:[0,&quot;2023-12-16-cdk-notifier-feature-stacks&quot;],&quot;body&quot;:[0,&quot;\n## Use case\n\nEspecially in serverless environments, features will be created with ephemeral stacks, which will be deleted after merging to the main branch. Comparing the cdk diff between the feature branch and the main branch will help to understand which changes will be applied after merging the pull request.\n\nSome changes, like renaming a dynamodb table or updating more than one dynamodb index, will work if the changes are made before deploying the feature stack but fail if the changes are applied to an existing stack.\n\nThe cdk-notifer will help to identify these changes.\n\n## cdk-notifier\n\nSee here for more information about the cdk-notifier:\n[https://github.com/karlderkaefer/cdk-notifier](https://github.com/karlderkaefer/cdk-notifier)\n\n## GitHub Action\n\nThis setting has a deployed stack from the main branch, for the feature development a new branch `feat1` is created and deployed as a new stack.\n\n![stack-overview]({{ site.baseurl }}/img/2023-12-16-cdk-notifier-feature-stacks/stack-overview.png)\n\nIn the pull request the cdk-notifier action is added to the workflow and create a diff from the `feat1` branch files with the BRANCH_NAME env variable for `main` to run the diff agianst the `main` stack.\nThe diff output will be added to the pull request as a comment via the cdk-notifier.\n\n```yaml\n      - name: Check diff to main\n        run: |\n          npm ci  \n          echo \&quot;check the diff to main\&quot;\n          BRANCH_NAME=main npx cdk diff --app cdk.out --progress=events &amp;&gt; &gt;(tee cdk.log)\n          echo \&quot;create cdk-notifier report\&quot;\n          echo \&quot;BRANCH_NAME: $BRANCH_NAME\&quot;\n          echo \&quot;GITHUB_OWNER: $GITHUB_OWNER\&quot;\n          echo \&quot;GITHUB_REPO: $GITHUB_REPO\&quot;\n          cdk-notifier \\\n          --owner ${{ env.GITHUB_OWNER }} \\\n          --repo ${{ env.GITHUB_REPO }} \\\n          --token ${{ secrets.GITHUB_TOKEN }} \\\n          --log-file ./cdk.log \\\n          --tag-id diff-to-main \\\n          --pull-request-id ${{ env.PULL_REQUEST_ID }} \\\n          --vcs github \\\n          --ci circleci \\\n          --template extendedWithResources\n```\n[https://github.com/JohannesKonings/cdk-notifier-examples/blob/main/.github/workflows/cdk-diff.yml](https://github.com/JohannesKonings/cdk-notifier-examples/blob/main/.github/workflows/cdk-diff.yml)\n\n![pipeline in pull request]({{ site.baseurl }}/img/2023-12-16-cdk-notifier-feature-stacks/pipeline-in-pull-request.png)\n\nThe comment looks like this:\n\n![PR comment]({{ site.baseurl }}/img/2023-12-16-cdk-notifier-feature-stacks/pr-comment.png)\n[https://github.com/JohannesKonings/cdk-notifier-examples/pull/4#issuecomment-1858909147](https://github.com/JohannesKonings/cdk-notifier-examples/pull/4#issuecomment-1858909147)\n\nThe extended template parameter of the cdk-notifier displays in this setting also a warning that the dynamodb requires a replacement.\nIn some cases, this could be unintended and be reverted securely in the feature branch before merging.\nThat is a nice feature besides the overview of the changes, which is itself helpful in making these changes more visible.\n\nAfter the merge, the feature stack will be destroyed, and the main stack will be updated.\n\n![pipeline after merge]({{ site.baseurl }}/img/2023-12-16-cdk-notifier-feature-stacks/pipeline-after-merge.png)\n\n\n## Code\n\n[https://github.com/JohannesKonings/cdk-notifier-examples](https://github.com/JohannesKonings/cdk-notifier-examples)\n\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;Use cdk-notifier to compare changes in pull requests&quot;],&quot;summary&quot;:[0,&quot;See how cdk-notifier can help to compare which cdk changes will be applied after merging a pull request&quot;],&quot;date&quot;:[3,&quot;2023-12-16T07:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;aws&quot;],[0,&quot;cdk&quot;],[0,&quot;cdk-notifier&quot;]]]}],&quot;render&quot;:[0,null]}],[0,{&quot;id&quot;:[0,&quot;2024-04-20-cdk-notifier-and-tags.md&quot;],&quot;slug&quot;:[0,&quot;2024-04-20-cdk-notifier-and-tags&quot;],&quot;body&quot;:[0,&quot;\n## Use case\n\nAs described here [Use cdk-notifier to compare changes in pull requests]({{ site.baseurl }}/aws/cdk/2023/12/16/cdk-notifier-feature-stacks.html), the cdk-notifier displays the diff between the feature branch and the main branch.\nIn case of using tags in the CDK there a two ways to tag resources, which will have different consequences in the diff output of the cdk-notifier.\n\n## Tagging with Tags.of()\n\nThe documentation of CDK describes the tagging of resources with the `Tags.of()` method: https://docs.aws.amazon.com/cdk/v2/guide/tagging.html\nThis could look like this:  \n```typescript\nTags.of(app).add(&#39;branch&#39;, branchName);\n```\nhttps://github.com/JohannesKonings/cdk-notifier-examples/blob/746c2b2bc0ecc0ecf3e8f0e6ff771a7430a45d04/src/main.ts#L23\n\nThe tag will then be added to all resources in the synthesized cloudformation template.\n\n```JSON\n{\n \&quot;Resources\&quot;: {\n  \&quot;TableCD117FA1\&quot;: {\n   \&quot;Type\&quot;: \&quot;AWS::DynamoDB::Table\&quot;,\n   \&quot;Properties\&quot;: {\n    \&quot;AttributeDefinitions\&quot;: [\n     {\n      \&quot;AttributeName\&quot;: \&quot;id\&quot;,\n      \&quot;AttributeType\&quot;: \&quot;S\&quot;\n     }\n    ],\n    \&quot;BillingMode\&quot;: \&quot;PAY_PER_REQUEST\&quot;,\n    \&quot;KeySchema\&quot;: [\n     {\n      \&quot;AttributeName\&quot;: \&quot;id\&quot;,\n      \&quot;KeyType\&quot;: \&quot;HASH\&quot;\n     }\n    ],\n    \&quot;TableName\&quot;: \&quot;Table-tags-tags-of\&quot;,\n    \&quot;Tags\&quot;: [\n     {\n      \&quot;Key\&quot;: \&quot;branch\&quot;,\n      \&quot;Value\&quot;: \&quot;tags-tags-of\&quot;\n     }\n    ]\n   },\n   ...\n  }\n }\n}\n```\nBecause the tag is in the template, it will then be shown in the diff.\n\n![diff tag of]({{ site.baseurl }}/img/2024-04-20-cdk-notifier-and-tags/diff-tag-of.png)\n\nhttps://github.com/JohannesKonings/cdk-notifier-examples/pull/5\n\n## Tagging with stack properties\n\nThe other way is to pass the tags as stack properties (https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.Stack.html#tags-1).\nThis could look like this:\n```typescript\nnew CdkNotfifierFeatureStackExample(app, `cdk-notifier-feature-stacks-${branchName}`, {\n  tags: {\n    branch: branchName,\n  },\n});\n```\nhttps://github.com/JohannesKonings/cdk-notifier-examples/blob/66874c06b8204b09781e9ad3ab8707590b948000/src/main.ts#L23\n\nThe tag will then be added to the stack properties and not to the template file.\n\n```JSON\n{\n \&quot;Resources\&quot;: {\n  \&quot;TableCD117FA1\&quot;: {\n   \&quot;Type\&quot;: \&quot;AWS::DynamoDB::Table\&quot;,\n   \&quot;Properties\&quot;: {\n    \&quot;AttributeDefinitions\&quot;: [\n     {\n      \&quot;AttributeName\&quot;: \&quot;id\&quot;,\n      \&quot;AttributeType\&quot;: \&quot;S\&quot;\n     }\n    ],\n    \&quot;BillingMode\&quot;: \&quot;PAY_PER_REQUEST\&quot;,\n    \&quot;KeySchema\&quot;: [\n     {\n      \&quot;AttributeName\&quot;: \&quot;id\&quot;,\n      \&quot;KeyType\&quot;: \&quot;HASH\&quot;\n     }\n    ],\n    \&quot;TableName\&quot;: \&quot;Table-tags-stack-properties\&quot;,\n   },\n   ...\n  }\n }\n}\n```\n\nIn `cdk.out` the tags are only in the `manifest.json` file.\n\n```JSON\n{\n  \&quot;version\&quot;: \&quot;36.0.0\&quot;,\n  \&quot;artifacts\&quot;: {\n    \&quot;cdk-notifier-feature-stacks-tags-stack-properties.assets\&quot;: {\n      \&quot;type\&quot;: \&quot;cdk:asset-manifest\&quot;,\n      \&quot;properties\&quot;: {\n        \&quot;file\&quot;: \&quot;cdk-notifier-feature-stacks-tags-stack-properties.assets.json\&quot;,\n        \&quot;requiresBootstrapStackVersion\&quot;: 6,\n        \&quot;bootstrapStackVersionSsmParameter\&quot;: \&quot;/cdk-bootstrap/hnb659fds/version\&quot;\n      }\n    },\n    \&quot;cdk-notifier-feature-stacks-tags-stack-properties\&quot;: {\n      \&quot;type\&quot;: \&quot;aws:cloudformation:stack\&quot;,\n      \&quot;environment\&quot;: \&quot;aws://unknown-account/unknown-region\&quot;,\n      \&quot;properties\&quot;: {\n        \&quot;templateFile\&quot;: \&quot;cdk-notifier-feature-stacks-tags-stack-properties.template.json\&quot;,\n        \&quot;terminationProtection\&quot;: false,\n        \&quot;tags\&quot;: {\n          \&quot;branch\&quot;: \&quot;tags-stack-properties\&quot;\n        },\n        \&quot;validateOnSynth\&quot;: false,\n        ...\n      }\n    }\n  }\n}\n```\n\nThen it will not be shown in the diff, and the cdk-notifier skip the pull request comment.\n\n```bash\ncheck the diff to main\nDeploying with stack postfix main\nStack cdk-notifier-feature-stacks-main\nHold on while we create a read-only change set to get a diff with accurate replacement information (use --no-change-set to use a less accurate but faster template-only diff)\nThere were no differences\n\n✨  Number of stacks with differences: 0\n\ncreate cdk-notifier report\nBRANCH_NAME: tags-stack-properties\nGITHUB_OWNER: JohannesKonings\nGITHUB_REPO: $(echo JohannesKonings/cdk-notifier-examples | cut -d&#39;/&#39; -f2)\ntime=\&quot;2024-04-20T14:59:48Z\&quot; level=info msg=\&quot;There is no diff detected for tag id diff-to-main. Skip posting diff.\&quot;\n```\nhttps://github.com/JohannesKonings/cdk-notifier-examples/actions/runs/8765869174/job/24057331666#step:6:55\n\n## Conclusion\n\nIf you want to see the tags in the diff output of the cdk-notifier, you should use the `Tags.of()` method to tag the resources.\nIf not, you can go with the stack properties.\n\n## Code\n\n[https://github.com/JohannesKonings/cdk-notifier-examples](https://github.com/JohannesKonings/cdk-notifier-examples)\n\n&quot;],&quot;collection&quot;:[0,&quot;blog&quot;],&quot;data&quot;:[0,{&quot;published&quot;:[0,true],&quot;title&quot;:[0,&quot;Consideration about cdk-notifier and Tags&quot;],&quot;summary&quot;:[0,&quot;Description of the diff consequence of two ways of tagging in CDK&quot;],&quot;date&quot;:[3,&quot;2024-04-20T08:15:18.000Z&quot;],&quot;tags&quot;:[1,[[0,&quot;aws&quot;],[0,&quot;cdk&quot;],[0,&quot;cdk-notifier&quot;]]]}],&quot;render&quot;:[0,null]}]]]}" ssr="" client="load" opts="{&quot;name&quot;:&quot;Search&quot;,&quot;value&quot;:true}" await-children=""><div data-hk="s00-0-0-0" class="flex flex-col"><div class="relative"><input name="search" type="text" value="" autocomplete="off" spellcheck="false" placeholder="What are you looking for?" class="w-full px-2.5 py-1.5 pl-10 rounded outline-none text-black dark:text-white bg-black/5 dark:bg-white/15 border border-black/10 dark:border-white/20 focus:border-black focus:dark:border-white"><svg class="absolute size-6 left-1.5 top-1/2 -translate-y-1/2 stroke-current"><use href="/ui.svg#search"></use></svg></div><!--$--><!--/--></div><!--astro:end--></astro-island> </div>   </div> </div>  </main> <footer class="relative bg-white dark:bg-black"> <div class="animate"> <section class="py-5"> <div class="w-full h-full mx-auto px-5 max-w-screen-md">  <div class="flex items-center justify-center sm:justify-end"> <button id="back-to-top" aria-label="Back to top of page" class="group flex w-fit p-1.5 gap-1.5 text-sm items-center border rounded hover:bg-black/5 hover:dark:bg-white/10 border-black/15 dark:border-white/20 transition-colors duration-300 ease-in-out"> <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round" class="stroke-current group-hover:stroke-black group-hover:dark:stroke-white rotate-90"> <line x1="19" y1="12" x2="5" y2="12" class="scale-x-0 group-hover:scale-x-100 translate-x-3 group-hover:translate-x-0 transition-all duration-300 ease-in-out"></line> <polyline points="12 19 5 12 12 5" class="translate-x-1 group-hover:translate-x-0 transition-all duration-300 ease-in-out"></polyline> </svg> <div class="w-full group-hover:text-black group-hover:dark:text-white transition-colors duration-300 ease-in-out">
Back to top
</div> </button> </div>  </div> </section> <section class="py-5 overflow-hidden whitespace-nowrap border-t border-black/10 dark:border-white/25"> <div class="w-full h-full mx-auto px-5 max-w-screen-md">  <div class="h-full grid grid-cols-1 sm:grid-cols-2 gap-3"> <div class="order-2 sm:order-1 flex flex-col items-center justify-center sm:items-start"> <div class="legal"> <a href="/legal/terms" class="text-current hover:text-black dark:hover:text-white transition-colors duration-300 ease-in-out">
Terms
</a> |
<a href="/legal/privacy" class="text-current hover:text-black dark:hover:text-white transition-colors duration-300 ease-in-out">
Privacy
</a> </div> <div class="text-sm mt-2">&copy; 2024 | All rights reserved</div> </div> <div class="order-1 sm:order-2 flex justify-center sm:justify-end"> <div class="flex flex-wrap gap-1 items-center justify-center"> <!-- {
                SOCIALS.map((SOCIAL) => (
                  <a 
                    href={SOCIAL.HREF} 
                    target="_blank" 
                    aria-label={`${SITE.TITLE} on ${SOCIAL.NAME}`} 
                    class="group size-10 rounded-full p-2 items-center justify-center hover:bg-black/5 dark:hover:bg-white/20  blend"
                  >
                    <svg class="size-full fill-current group-hover:fill-black group-hover:dark:fill-white blend">
                      <use href={`/social.svg#${SOCIAL.ICON}`} />
                    </svg>
                  </a>
                ))
              } --> <a href="https://github.com/JohannesKonings/JohannesKonings.github.io" target="_blank" aria-label="GitHub repo" class="group size-10 rounded-full p-2 items-center justify-center hover:bg-black/5 dark:hover:bg-white/20 blend"> <svg class="size-full fill-current group-hover:fill-black group-hover:dark:fill-white blend"> <use href="/social.svg#github"></use> </svg> </a> </div> </div> </div>  </div> </section> </div> </footer> <script>
  function goBackToTop(event) {
    event.preventDefault();
    window.scrollTo({
      top: 0,
      behavior: "smooth",
    });
  }

  function inintializeBackToTop() {
    const backToTop = document.getElementById("back-to-top");
    backToTop?.addEventListener("click", goBackToTop);
  }

  document.addEventListener("astro:after-swap", inintializeBackToTop);
  inintializeBackToTop();
</script> </body></html>